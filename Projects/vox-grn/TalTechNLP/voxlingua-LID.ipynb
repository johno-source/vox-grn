{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "language_id = EncoderClassifier.from_hparams(source=\"TalTechNLP/voxlingua107-epaca-tdnn\", savedir=\"tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.3233, 0.3781, 0.3702, 0.3957, 0.4054, 0.3674, 0.3713, 0.3625, 0.3535,\n",
      "         0.3686, 0.3917, 0.3999, 0.3873, 0.3989, 0.3973, 0.3614, 0.4387, 0.4022,\n",
      "         0.4131, 0.3912, 0.4198, 0.3893, 0.3790, 0.3792, 0.3701, 0.4023, 0.4283,\n",
      "         0.4124, 0.3587, 0.3723, 0.3339, 0.3863, 0.3875, 0.3888, 0.3902, 0.3966,\n",
      "         0.3952, 0.4090, 0.3831, 0.3856, 0.3015, 0.4217, 0.4006, 0.3678, 0.3982,\n",
      "         0.3768, 0.4323, 0.3833, 0.3640, 0.4731, 0.3553, 0.4179, 0.3674, 0.3799,\n",
      "         0.3840, 0.6093, 0.3735, 0.3913, 0.3794, 0.4042, 0.3811, 0.3629, 0.4219,\n",
      "         0.3742, 0.4428, 0.3267, 0.3888, 0.3893, 0.4132, 0.4321, 0.4202, 0.3384,\n",
      "         0.3621, 0.3469, 0.3579, 0.3799, 0.4008, 0.3806, 0.2410, 0.4138, 0.4040,\n",
      "         0.4092, 0.3293, 0.3677, 0.3911, 0.3932, 0.3785, 0.3657, 0.4482, 0.3493,\n",
      "         0.3923, 0.3686, 0.3903, 0.3796, 0.9301, 0.3713, 0.4322, 0.4238, 0.3864,\n",
      "         0.3547, 0.3531, 0.3495, 0.4793, 0.4070, 0.3873, 0.3998, 0.4365]]), tensor([0.9301]), tensor([94]), ['th'])\n"
     ]
    }
   ],
   "source": [
    "# Download Thai language sample from Omniglot and cvert to suitable form\n",
    "signal = language_id.load_audio(\"https://omniglot.com/soundfiles/udhr/udhr_th.mp3\")\n",
    "prediction =  language_id.classify_batch(signal)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['th']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The scores in the prediction[0] tensor can be interpreted as cosine scores between\n",
    "# the languages and the given utterance (i.e., the larger the better)\n",
    "# The identified language ISO code is given in prediction[3]\n",
    "print(prediction[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, use the utterance embedding extractor:\n",
    "emb =  language_id.encode_batch(signal)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset vox_grn (/home/jovyan/.cache/huggingface/datasets/sil-ai___vox_grn/spa/0.0.0/d972908cdd1882272fc938c772340dfd500306b1611ad91be1de6f05f28357fd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da27dfd7f51942c394dc0a4db0ac1894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sil-ai/VoxGRN\", \"spa\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/huggingface/datasets/downloads/extracted/8f7f64e4c88f713ef22b73b6d96a3b6830ee06c216132de76baa9a4ae3a4a01c/Audio_MP3/19/19990/Espanol Good News^ 027 Picture 25 The Holy Spirit Comes 19990.mp3\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['audio']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.5073, 0.5590, 0.5245, 0.5400, 0.5515, 0.5525, 0.5476, 0.5643, 0.5783,\n",
      "         0.5279, 0.5343, 0.5811, 0.5796, 0.8183, 0.5840, 0.5626, 0.5673, 0.5259,\n",
      "         0.5575, 0.6223, 0.5767, 0.6121, 0.9585, 0.5707, 0.8042, 0.5253, 0.5855,\n",
      "         0.5317, 0.5481, 0.8063, 0.6375, 0.5368, 0.4841, 0.5354, 0.5168, 0.5191,\n",
      "         0.5865, 0.5639, 0.5542, 0.5025, 0.5969, 0.5558, 0.5864, 0.6023, 0.5924,\n",
      "         0.5761, 0.5714, 0.5239, 0.5389, 0.5172, 0.5292, 0.5538, 0.5875, 0.5106,\n",
      "         0.5503, 0.5036, 0.5563, 0.5467, 0.5521, 0.4983, 0.5845, 0.5279, 0.5567,\n",
      "         0.5278, 0.5748, 0.4972, 0.5128, 0.5257, 0.5352, 0.5648, 0.5651, 0.6557,\n",
      "         0.5023, 0.5691, 0.5091, 0.5955, 0.5922, 0.5789, 0.4447, 0.5028, 0.5379,\n",
      "         0.5454, 0.5676, 0.5789, 0.5407, 0.5374, 0.5613, 0.5178, 0.5615, 0.5618,\n",
      "         0.5512, 0.5487, 0.5374, 0.5353, 0.5320, 0.4984, 0.5594, 0.5250, 0.5738,\n",
      "         0.5405, 0.5363, 0.5648, 0.5481, 0.5346, 0.5458, 0.5175, 0.5215]]), tensor([0.9585]), tensor([22]), ['es'])\n"
     ]
    }
   ],
   "source": [
    "sig = language_id.load_audio(dataset['train'][0]['audio']['path'])\n",
    "prediction =  language_id.classify_batch(sig)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es']\n"
     ]
    }
   ],
   "source": [
    "print(prediction[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2085ca88d9373de1\n",
      "Reusing dataset json (/home/jovyan/.cache/huggingface/datasets/json/default-2085ca88d9373de1/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "vox6_ds = load_dataset('json', data_files='../../../data/vox6.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e17ffc479af4f7ba235d48bbb05da19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def map_to_array(example):\n",
    "    example[\"speech\"] = language_id.load_audio(example[\"file\"])\n",
    "    return example\n",
    "\n",
    "# load a demo dataset and read audio files\n",
    "vox6 = vox6_ds.map(map_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/vox-grn/Projects/vox-grn/TalTechNLP/voxlingua-LID.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvox-grn/home/jovyan/work/vox-grn/Projects/vox-grn/TalTechNLP/voxlingua-LID.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m prediction \u001b[39m=\u001b[39m  language_id\u001b[39m.\u001b[39;49mclassify_batch(vox6[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mspeech\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/speechbrain/pretrained/interfaces.py:866\u001b[0m, in \u001b[0;36mEncoderClassifier.classify_batch\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassify_batch\u001b[39m(\u001b[39mself\u001b[39m, wavs, wav_lens\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    838\u001b[0m     \u001b[39m\"\"\"Performs classification on the top of the encoded features.\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \n\u001b[1;32m    840\u001b[0m \u001b[39m    It returns the posterior probabilities, the index and, if the label\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39m        (label encoder should be provided).\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 866\u001b[0m     emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_batch(wavs, wav_lens)\n\u001b[1;32m    867\u001b[0m     out_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmods\u001b[39m.\u001b[39mclassifier(emb)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    868\u001b[0m     score, index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(out_prob, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/speechbrain/pretrained/interfaces.py:816\u001b[0m, in \u001b[0;36mEncoderClassifier.encode_batch\u001b[0;34m(self, wavs, wav_lens, normalize)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m\"\"\"Encodes the input audio into a single vector embedding.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \n\u001b[1;32m    791\u001b[0m \u001b[39mThe waveforms should already be in the model's desired format.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39m    The encoded batch\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m# Manage single waveforms in input\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(wavs\u001b[39m.\u001b[39;49mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    817\u001b[0m     wavs \u001b[39m=\u001b[39m wavs\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[39m# Assign full length if wav_lens is not assigned\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "prediction =  language_id.classify_batch(vox6[0]['speech'])\n",
    "# print(vox6[0]['speech'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
