{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment GRN data using pyAudioAnalysis\n",
    "\n",
    "The group of notebooks SegmentPyVOXn.ipynb (n=0-9) will be used to segment the data using the segmentation algorithm I developed in DevelopASSegmentation.ipynb. It follows the same pattern as used in SegmentFVOXn.ipynb.\n",
    "\n",
    "# Data to be Created\n",
    "\n",
    "Three sets of data will be made, 4, 6, and 10 second data. \n",
    "\n",
    "This file will take a long time to run. For that reason it will check point its progress at regular intervals by writing smaller files. If its progress is restarted for any reason it will use those files to determine where it was up to. Segmenting the data is the slow part. For each audio file a segment file will be generated. If mp3 files are to be generated later using different parameters the segmented files should make the process much quicker.\n",
    "\n",
    "The input to this process is the same as SegmentFVOX.ipynb. We actually use its division of the files into 10 groups.\n",
    "\n",
    "The output will be:\n",
    "    \n",
    "    1. /media/originals/py_audio_seg/[iso]/[filename].pkl\n",
    "        where each pkl file contains the list of raw segments for the item.\n",
    "    2. /media/originals/py_audio_seg/seg_4sec.csv\n",
    "        all the metadata for the seg_4sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    3. /media/originals/py_audio_seg/seg_6sec.csv\n",
    "        all the metadata for the seg_6sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    4. /media/originals/py_audio_seg/seg_10sec.csv\n",
    "        all the metadata for the seg_10sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    5. /media/originals/datasets/py_audio_seg_4sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n",
    "    6. /media/originals/datasets/py_audio_seg_6sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n",
    "    7. /media/originals/datasets/py_audio_seg_10sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('~/work/pyAudioAnalysis'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from collections import namedtuple\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "from pyAudioAnalysis import audioTrainTest as at\n",
    "from pyAudioAnalysis import MidTermFeatures as mtf\n",
    "from pyAudioAnalysis import audioBasicIO as aIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the locations for each of the file types\n",
    "NOTEBOOK_ID=6\n",
    "SEGMENTS_DIR = '/media/originals/py_audio_seg/'\n",
    "DATASETS_DIR = '/media/originals/datasets/'\n",
    "SEC_4_DATA_DIR = 'py_audio_seg_4sec/data/'\n",
    "SEC_6_DATA_DIR = 'py_audio_seg_6sec/data/'\n",
    "SEC_10_DATA_DIR = 'py_audio_seg_10sec/data/'\n",
    "\n",
    "# define specific files used in the process\n",
    "SEG_4_SEC_DF = f'{SEGMENTS_DIR}seg_4sec_{NOTEBOOK_ID}.csv'\n",
    "SEG_6_SEC_DF = f'{SEGMENTS_DIR}seg_6sec_{NOTEBOOK_ID}.csv'\n",
    "SEG_10_SEC_DF = f'{SEGMENTS_DIR}seg_10sec_{NOTEBOOK_ID}.csv'\n",
    "\n",
    "# define segment sizes for each dataset\n",
    "SEG_4_SEC = 4.0\n",
    "SEG_6_SEC = 6.0\n",
    "SEG_10_SEC = 10.0\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "def convert_to_ms(sec):\n",
    "    return int(sec*1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'iso', 'language_name', 'track', 'location', 'year',\n",
       "       'path', 'filename', 'length', 'program'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now read in the description of the input and remove the unwanted columns and rename the rest to be python attribute names.\n",
    "fd = pd.read_csv(f'/media/originals/fsegs/files_{NOTEBOOK_ID}.csv')\n",
    "fd.set_index('ID', inplace=True)\n",
    "fd.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20841\n"
     ]
    }
   ],
   "source": [
    "# augment the fd with 1/3 of files 9\n",
    "fd9 = pd.read_csv(f'/media/originals/fsegs/files_9.csv')\n",
    "fd9.set_index('ID', inplace=True)\n",
    "print(len(fd9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = pd.concat([fd, fd9.iloc[:int(len(fd9)/3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                  204638\n",
      "iso                                                            nlj\n",
      "language_name                                        Kinyali: Kilo\n",
      "track                                                            2\n",
      "location                                                     Lolwa\n",
      "year                                                        1978.0\n",
      "path                                    vox_grn/Audio_MP3/16/16800\n",
      "filename         Kinyali Kilo Words of Life 1 002 Instrumental ...\n",
      "length                                                     838.968\n",
      "program                                                      16800\n",
      "Name: 16800_002, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fd.iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['65174_011', '65856_038', '65174_023', '65856_036', '65856_017',\n",
      "       '65856_016', '65856_015', '65856_037', '65856_013', '65174_024',\n",
      "       ...\n",
      "       '60065_003', '60065_007', '60054_010', '60065_009', '60065_010',\n",
      "       '60065_006', '60065_001', '60065_008', '31531_001', '31531_002'],\n",
      "      dtype='object', name='ID', length=27784)\n"
     ]
    }
   ],
   "source": [
    "print(fd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate directories and filenames\n",
    "def prepare_dir(dirname):\n",
    "    if dirname[-1] != '/':\n",
    "        dirname = dirname + '/'\n",
    "    Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "    return dirname\n",
    "\n",
    "def prepare_raw_seg_dir(fd):\n",
    "    return prepare_dir(SEGMENTS_DIR + fd.iso)\n",
    "\n",
    "def raw_seg_filename(fd):\n",
    "    return f'{fd.filename}.pkl'\n",
    "\n",
    "def prepare_dataset_data_dir(fd, dataset_dir):\n",
    "    return prepare_dir(DATASETS_DIR + dataset_dir + fd.iso)\n",
    "\n",
    "def seg_mp3_filename(fd, seg):\n",
    "    return f'{fd.filename[:-4]}_{seg:03d}.mp3'\n",
    "\n",
    "def get_fname(fd):\n",
    "    path = fd.path\n",
    "    if path[-1] != '/':\n",
    "        path = path + '/'\n",
    "    files = glob.glob('/media/programs/' + path + fd.filename.replace('\\ufffd', '*'))\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    return '/media/programs/' + path + fd.filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def condition_audio_segment(audio_seg):\n",
    "    if audio_seg.channels != 1:\n",
    "        audio_seg = audio_seg.set_channels(1)\n",
    "\n",
    "    if audio_seg.sample_width != 2:\n",
    "        audio_seg = audio_seg.set_sample_width(2)\n",
    "\n",
    "    if audio_seg.frame_rate != SAMPLING_RATE:\n",
    "        audio_seg = audio_seg.set_frame_rate(SAMPLING_RATE)\n",
    "    return audio_seg\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# sklearn puts out a lot of annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Segment = namedtuple('Segment', ['start', 'end', 'classification'])\n",
    "\n",
    "# re-implement a simplified version of mid_term_file_classification\n",
    "# it is implemented as a class to allow the model to be cached.\n",
    "class ExtractVoiceSegments():\n",
    "    classifier, mean, std, class_names, mt_win, mid_step, st_win, \\\n",
    "         st_step, compute_beat = at.load_model('/home/jovyan/work/pyAudioAnalysis/pyAudioAnalysis/data/models/svm_rbf_4class')\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def segments_in(self, signal, sampling_rate, offset):\n",
    "        labels = []\n",
    "\n",
    "        # mid-term feature extraction:\n",
    "        mt_feats, _, _ = \\\n",
    "            mtf.mid_feature_extraction(signal, sampling_rate,\n",
    "                                    ExtractVoiceSegments.mt_win * sampling_rate,\n",
    "                                    ExtractVoiceSegments.mid_step * sampling_rate,\n",
    "                                    round(sampling_rate * ExtractVoiceSegments.st_win),\n",
    "                                    round(sampling_rate * ExtractVoiceSegments.st_step))\n",
    "\n",
    "        # for each feature vector (i.e. for each fix-sized segment):\n",
    "        for col_index in range(mt_feats.shape[1]):\n",
    "            # normalize current feature v\n",
    "            feature_vector = (mt_feats[:, col_index] - ExtractVoiceSegments.mean) / ExtractVoiceSegments.std\n",
    "\n",
    "            # classify vector:\n",
    "            label_predicted, _ = \\\n",
    "                at.classifier_wrapper(ExtractVoiceSegments.classifier, 'svm', feature_vector)\n",
    "            labels.append(label_predicted)\n",
    "\n",
    "        segs, classes = aS.labels_to_segments(labels, ExtractVoiceSegments.mid_step)\n",
    "        # there is a bug in labels to segments when there is a single label. In this case it returns a list rather than a list of lists\n",
    "        if len(labels) == 1:\n",
    "            segs = [].append(segs)\n",
    "        return [] if segs is None else [Segment(seg[0]+offset, seg[1]+offset, ExtractVoiceSegments.class_names[int(label)]) for seg, label in zip(segs, classes)]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function performs mid-term classification of an audio stream.\n",
    "Towards this end, supervised knowledge is used,\n",
    "i.e. a pre-trained classifier.\n",
    "ARGUMENTS:\n",
    "    - input_file:        path of the input WAV/mp3 file\n",
    "RETURNS:\n",
    "    - list of Segments (see above tuple)\n",
    "\"\"\"\n",
    "def extract_voice_segments(input_file, *, __extract_voice_segments=ExtractVoiceSegments()):\n",
    "    segments = []\n",
    "\n",
    "    # load input file\n",
    "    sampling_rate, signal = aIO.read_audio_file(input_file)\n",
    "\n",
    "    # could not read file\n",
    "    if sampling_rate == 0:\n",
    "        return segments\n",
    "\n",
    "    # convert stereo (if) to mono\n",
    "    signal = aIO.stereo_to_mono(signal)\n",
    "\n",
    "    # find the silence segments\n",
    "    non_silent_segments = aS.silence_removal(signal, sampling_rate, 0.02, 0.02, smooth_window=1.0, weight=0.3)\n",
    "\n",
    "    # work through each segment\n",
    "    for seg in non_silent_segments:\n",
    "        start = int(seg[0]*sampling_rate)\n",
    "        stop = int(seg[1]*sampling_rate)\n",
    "        sig = signal[start:stop]\n",
    "\n",
    "        segments.extend(__extract_voice_segments.segments_in(signal[start:stop], sampling_rate, seg[0]))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = namedtuple('Epoch', ['start', 'end'])\n",
    "def speech_epochs_from_segments(segments, epoch_length=4.0, silence_tolerance=0.0):\n",
    "    epochs = []\n",
    "    i = 0\n",
    "    silence_this_epoch = silence_tolerance\n",
    "    while i < len(segments):\n",
    "        seg_duration = segments[i].end - segments[i].start\n",
    "        if segments[i].classification != 'speech':\n",
    "            silence_this_epoch = silence_tolerance\n",
    "\n",
    "        elif seg_duration >= epoch_length:\n",
    "            epochs.append(Epoch(segments[i].start, segments[i].start+epoch_length))\n",
    "            silence_this_epoch = silence_tolerance\n",
    "            # process the same segment again with a smaller size\n",
    "            new_start = segments[i].start+epoch_length\n",
    "            new_end = segments[i].end\n",
    "            if new_start < new_end:\n",
    "                segments[i] = Segment(new_start, new_end, segments[i].classification)\n",
    "                continue\n",
    "        else:\n",
    "            if i+1 < len(segments):\n",
    "                if (segments[i].end + silence_this_epoch) >= segments[i+1].start and segments[i+1].classification == 'speech':\n",
    "                    # did we use up any silence tolerence\n",
    "                    if segments[i].end < segments[i+1].start:\n",
    "                        silence_this_epoch -= (segments[i+1].start - segments[i].end)\n",
    "                    segments[i+1] = Segment(segments[i].start, segments[i+1].end, segments[i].classification)\n",
    "                else:\n",
    "                    silence_this_epoch = silence_tolerance\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_the_segment_info(fd, segs):\n",
    "    fname = prepare_raw_seg_dir(fd) + raw_seg_filename(fd)\n",
    "    with open(fname, 'wb') as pklFile:\n",
    "         pkl.dump(segs, pklFile)\n",
    "\n",
    "def read_the_segment_info(fd):\n",
    "    fname = prepare_raw_seg_dir(fd) + raw_seg_filename(fd)\n",
    "    if os.path.exists(fname):\n",
    "        if os.path.getsize(fname) > 0:\n",
    "            with open(fname, 'rb') as pklFile:\n",
    "                return pkl.load(pklFile)\n",
    "    return []\n",
    "    \n",
    "def update_dataframes(seg_df_csv, seg_records):\n",
    "    # now update the dataframes\n",
    "    if len(seg_records) > 0:\n",
    "        if os.path.isfile(seg_df_csv):\n",
    "            seg_sec_df = pd.concat([pd.read_csv(seg_df_csv, index_col='file_name'), pd.DataFrame.from_records(seg_records, index='file_name')])\n",
    "        else:\n",
    "            seg_sec_df = pd.DataFrame.from_records(seg_records, index='file_name')\n",
    "        seg_sec_df.to_csv(seg_df_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_segments_for_file(fd):\n",
    "    fmt = 'wav'\n",
    "    if fd.filename[-4:].lower() == '.mp3' :\n",
    "        fmt = 'mp3'\n",
    "    audio_seg = AudioSegment.from_file(get_fname(fd), format=fmt)\n",
    "\n",
    "    # now condition the segment and extract the raw segments.\n",
    "    audio_seg = condition_audio_segment(audio_seg)\n",
    "    segs = read_the_segment_info(fd)\n",
    "    if len(segs) == 0:\n",
    "        segs = extract_voice_segments(get_fname(fd))\n",
    "        save_the_segment_info(fd, segs)\n",
    "\n",
    "    return audio_seg, segs\n",
    "\n",
    "\n",
    "def create_mp3_files(audio_seg, segs, time_per_segment, dataset_dir, fd):\n",
    "    epochs_for_time = speech_epochs_from_segments(segs, epoch_length=time_per_segment, silence_tolerance=time_per_segment/4.0)\n",
    "\n",
    "    # now write out the 4 sec segments\n",
    "    dirname = prepare_dataset_data_dir(fd, dataset_dir)\n",
    "    rows = list()\n",
    "\n",
    "    for i, seg in enumerate(epochs_for_time):\n",
    "        file_name = dataset_dir + fd.iso + '/' + seg_mp3_filename(fd, i)\n",
    "        fname = dirname + seg_mp3_filename(fd, i)\n",
    "        start = convert_to_ms(seg.start)\n",
    "        stop = convert_to_ms(seg.end)\n",
    "        if not os.path.exists(fname):\n",
    "            audio_seg[start:stop].export(fname, format='mp3', bitrate='32k')\n",
    "        desc = dict(fd._asdict())\n",
    "        desc['seg_start'] = start\n",
    "        desc['seg_stop'] = stop\n",
    "        desc['seg'] = i\n",
    "        desc['file_name'] = file_name\n",
    "        rows.append(desc)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing these items might take a very long time. To permit the process to be interrupted and restarted the indexes of processed items\n",
    "# are maintained in a set that is pickled on each batch. This allows the batch to quickly pick up where it left off.\n",
    "def process_record_batch(files_df, *, batch_size=10):\n",
    "    batch_proc = 0\n",
    "    processed_file = f'{SEGMENTS_DIR}processed16_{NOTEBOOK_ID}.pkl'\n",
    "    if os.path.isfile(processed_file):\n",
    "        with open(processed_file, 'rb') as pklFile:\n",
    "            processed_files = pkl.load(pklFile)\n",
    "    else:\n",
    "        processed_files = set()\n",
    "\n",
    "    segmented_4sec_segs = []\n",
    "    segmented_6sec_segs = []\n",
    "    segmented_10sec_segs = []\n",
    "\n",
    "    for fd in files_df.itertuples():\n",
    "        if batch_proc < batch_size:\n",
    "            if fd.Index not in processed_files:\n",
    "                try:\n",
    "                    audio_seg, voice_segs = extract_audio_segments_for_file(fd)\n",
    "\n",
    "                    segmented_4sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_4_SEC, SEC_4_DATA_DIR, fd))\n",
    "                    segmented_6sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_6_SEC, SEC_6_DATA_DIR, fd))\n",
    "                    segmented_10sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_10_SEC, SEC_10_DATA_DIR, fd))\n",
    "                except:\n",
    "                    print(f'exception {fd.filename}')\n",
    "                    pass\n",
    "\n",
    "                # we want to add an fd that has an exception so it is not reprocessed on every batch\n",
    "                processed_files.add(fd.Index)\n",
    "                batch_proc += 1\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    update_dataframes(SEG_4_SEC_DF, segmented_4sec_segs)\n",
    "    update_dataframes(SEG_6_SEC_DF, segmented_6sec_segs)\n",
    "    update_dataframes(SEG_10_SEC_DF, segmented_10sec_segs)\n",
    "\n",
    "    with open(processed_file, 'wb') as pklFile:\n",
    "        pkl.dump(processed_files, pklFile)\n",
    "\n",
    "    return processed_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12050 out of 27784 in 00:06:36\n",
      "Processed 12100 out of 27784 in 00:16:53\n",
      "Processed 12150 out of 27784 in 00:27:35\n",
      "Processed 12200 out of 27784 in 00:33:12\n",
      "Processed 12250 out of 27784 in 00:43:43\n",
      "Processed 12300 out of 27784 in 01:00:26\n",
      "Processed 12350 out of 27784 in 01:19:53\n",
      "Processed 12400 out of 27784 in 01:27:12\n",
      "Processed 12450 out of 27784 in 01:34:05\n",
      "Processed 12500 out of 27784 in 01:42:14\n",
      "Processed 12550 out of 27784 in 01:51:29\n",
      "Processed 12600 out of 27784 in 01:59:24\n",
      "Processed 12650 out of 27784 in 02:06:03\n",
      "Processed 12700 out of 27784 in 02:15:35\n",
      "Processed 12750 out of 27784 in 02:28:56\n",
      "Processed 12800 out of 27784 in 02:36:31\n",
      "Processed 12850 out of 27784 in 03:00:06\n",
      "Processed 12900 out of 27784 in 03:09:31\n",
      "Processed 12950 out of 27784 in 03:27:52\n",
      "Processed 13000 out of 27784 in 03:37:27\n",
      "Processed 13050 out of 27784 in 03:54:12\n",
      "Processed 13100 out of 27784 in 05:21:18\n",
      "Processed 13150 out of 27784 in 05:38:14\n",
      "Processed 13200 out of 27784 in 07:02:03\n",
      "Processed 13250 out of 27784 in 07:44:59\n",
      "Processed 13300 out of 27784 in 08:06:01\n",
      "Processed 13350 out of 27784 in 08:12:51\n",
      "Processed 13400 out of 27784 in 08:21:14\n",
      "Processed 13450 out of 27784 in 08:31:46\n",
      "Processed 13500 out of 27784 in 08:42:47\n",
      "Processed 13550 out of 27784 in 08:52:12\n",
      "Processed 13600 out of 27784 in 09:10:56\n",
      "Processed 13650 out of 27784 in 09:24:16\n",
      "Processed 13700 out of 27784 in 09:34:33\n",
      "Processed 13750 out of 27784 in 09:44:05\n",
      "Processed 13800 out of 27784 in 09:56:21\n",
      "Processed 13850 out of 27784 in 10:04:59\n",
      "Processed 13900 out of 27784 in 10:18:40\n",
      "Processed 13950 out of 27784 in 10:25:59\n",
      "Processed 14000 out of 27784 in 10:42:52\n",
      "Processed 14050 out of 27784 in 10:49:57\n",
      "Processed 14100 out of 27784 in 11:01:41\n",
      "Processed 14150 out of 27784 in 11:08:01\n",
      "Processed 14200 out of 27784 in 11:15:02\n",
      "Processed 14250 out of 27784 in 11:21:54\n",
      "Processed 14300 out of 27784 in 11:28:50\n",
      "Processed 14350 out of 27784 in 11:36:23\n",
      "Processed 14400 out of 27784 in 11:43:12\n",
      "Processed 14450 out of 27784 in 11:48:31\n",
      "Processed 14500 out of 27784 in 12:05:48\n",
      "Processed 14550 out of 27784 in 12:11:20\n",
      "Processed 14600 out of 27784 in 12:49:30\n",
      "Processed 14650 out of 27784 in 12:59:04\n",
      "Processed 14700 out of 27784 in 13:08:01\n",
      "Processed 14750 out of 27784 in 13:13:17\n",
      "Processed 14800 out of 27784 in 13:19:37\n",
      "Processed 14850 out of 27784 in 13:25:35\n",
      "Processed 14900 out of 27784 in 13:36:09\n",
      "Processed 14950 out of 27784 in 13:57:25\n",
      "Processed 15000 out of 27784 in 14:06:35\n",
      "Processed 15050 out of 27784 in 14:18:05\n",
      "Processed 15100 out of 27784 in 14:35:51\n",
      "Processed 15150 out of 27784 in 14:44:27\n",
      "Processed 15200 out of 27784 in 14:53:07\n",
      "Processed 15250 out of 27784 in 15:02:29\n",
      "Processed 15300 out of 27784 in 15:12:26\n",
      "Processed 15350 out of 27784 in 15:21:28\n",
      "Processed 15400 out of 27784 in 15:31:37\n",
      "Processed 15450 out of 27784 in 15:40:46\n",
      "Processed 15500 out of 27784 in 15:47:29\n",
      "Processed 15550 out of 27784 in 15:56:31\n",
      "Processed 15600 out of 27784 in 16:05:31\n",
      "Processed 15650 out of 27784 in 16:13:20\n",
      "Processed 15700 out of 27784 in 16:24:05\n",
      "Processed 15750 out of 27784 in 16:35:18\n",
      "Processed 15800 out of 27784 in 16:47:02\n",
      "Processed 15850 out of 27784 in 17:01:15\n",
      "Processed 15900 out of 27784 in 17:08:31\n",
      "Processed 15950 out of 27784 in 17:15:31\n",
      "Processed 16000 out of 27784 in 17:22:06\n",
      "Processed 16050 out of 27784 in 17:28:09\n",
      "Processed 16100 out of 27784 in 17:40:08\n",
      "Processed 16150 out of 27784 in 17:48:02\n",
      "Processed 16200 out of 27784 in 18:06:03\n",
      "Processed 16250 out of 27784 in 18:19:22\n",
      "Processed 16300 out of 27784 in 18:36:35\n",
      "Processed 16350 out of 27784 in 18:45:04\n",
      "Processed 16400 out of 27784 in 18:54:20\n",
      "Processed 16450 out of 27784 in 19:03:41\n",
      "Processed 16500 out of 27784 in 19:10:27\n",
      "Processed 16550 out of 27784 in 19:16:43\n",
      "Processed 16600 out of 27784 in 19:25:44\n",
      "Processed 16650 out of 27784 in 19:32:25\n",
      "Processed 16700 out of 27784 in 19:38:38\n",
      "Processed 16750 out of 27784 in 19:46:20\n",
      "Processed 16800 out of 27784 in 19:52:30\n",
      "Processed 16850 out of 27784 in 20:06:41\n",
      "Processed 16900 out of 27784 in 20:15:41\n",
      "Processed 16950 out of 27784 in 21:04:53\n",
      "Processed 17000 out of 27784 in 21:08:52\n",
      "Processed 17050 out of 27784 in 21:15:38\n",
      "Processed 17100 out of 27784 in 21:24:12\n",
      "Processed 17150 out of 27784 in 21:35:00\n",
      "Processed 17200 out of 27784 in 22:03:58\n",
      "Processed 17250 out of 27784 in 22:40:38\n",
      "Processed 17300 out of 27784 in 22:47:55\n",
      "Processed 17350 out of 27784 in 23:08:49\n",
      "Processed 17400 out of 27784 in 23:15:10\n",
      "Processed 17450 out of 27784 in 23:23:43\n",
      "Processed 17500 out of 27784 in 23:31:35\n",
      "Processed 17550 out of 27784 in 23:45:34\n",
      "Processed 17600 out of 27784 in 23:54:39\n",
      "Processed 17650 out of 27784 in 24:01:18\n",
      "Processed 17700 out of 27784 in 24:08:01\n",
      "Processed 17750 out of 27784 in 24:17:05\n",
      "Processed 17800 out of 27784 in 24:34:44\n",
      "Processed 17850 out of 27784 in 24:50:29\n",
      "Processed 17900 out of 27784 in 25:05:26\n",
      "Processed 17950 out of 27784 in 25:22:06\n",
      "Processed 18000 out of 27784 in 25:42:17\n",
      "Processed 18050 out of 27784 in 25:57:04\n",
      "Processed 18100 out of 27784 in 26:09:04\n",
      "Processed 18150 out of 27784 in 26:25:16\n",
      "Processed 18200 out of 27784 in 26:31:49\n",
      "Processed 18250 out of 27784 in 26:50:56\n",
      "Processed 18300 out of 27784 in 27:09:27\n",
      "Processed 18350 out of 27784 in 27:39:00\n",
      "Processed 18400 out of 27784 in 27:49:54\n",
      "Processed 18450 out of 27784 in 27:56:01\n",
      "Processed 18500 out of 27784 in 28:06:51\n",
      "Processed 18550 out of 27784 in 28:15:22\n",
      "Processed 18600 out of 27784 in 28:25:55\n",
      "Processed 18650 out of 27784 in 28:34:14\n",
      "Processed 18700 out of 27784 in 28:47:41\n",
      "Processed 18750 out of 27784 in 28:58:18\n",
      "Processed 18800 out of 27784 in 29:07:35\n",
      "Processed 18850 out of 27784 in 29:15:27\n",
      "Processed 18900 out of 27784 in 29:22:16\n",
      "Processed 18950 out of 27784 in 29:32:34\n",
      "Processed 19000 out of 27784 in 29:42:48\n",
      "Processed 19050 out of 27784 in 29:53:45\n",
      "Processed 19100 out of 27784 in 30:00:05\n",
      "Processed 19150 out of 27784 in 30:12:07\n",
      "Processed 19200 out of 27784 in 30:19:11\n",
      "Processed 19250 out of 27784 in 30:36:30\n",
      "Processed 19300 out of 27784 in 30:48:44\n",
      "Processed 19350 out of 27784 in 30:59:10\n",
      "Processed 19400 out of 27784 in 31:18:38\n",
      "Processed 19450 out of 27784 in 31:25:05\n",
      "Processed 19500 out of 27784 in 31:38:41\n",
      "Processed 19550 out of 27784 in 31:49:45\n",
      "Processed 19600 out of 27784 in 31:58:32\n",
      "Processed 19650 out of 27784 in 32:11:52\n",
      "Processed 19700 out of 27784 in 32:21:50\n",
      "Processed 19750 out of 27784 in 32:28:20\n",
      "Processed 19800 out of 27784 in 32:39:37\n",
      "Processed 19850 out of 27784 in 32:48:31\n",
      "Processed 19900 out of 27784 in 32:56:19\n",
      "Processed 19950 out of 27784 in 33:15:29\n",
      "Processed 20000 out of 27784 in 33:25:28\n",
      "Processed 20050 out of 27784 in 33:40:42\n",
      "Processed 20100 out of 27784 in 33:51:21\n",
      "Processed 20150 out of 27784 in 34:08:36\n",
      "Processed 20200 out of 27784 in 34:16:32\n",
      "Processed 20250 out of 27784 in 34:27:38\n",
      "Processed 20300 out of 27784 in 34:42:04\n",
      "Processed 20350 out of 27784 in 34:53:43\n",
      "Processed 20400 out of 27784 in 35:01:35\n",
      "Processed 20450 out of 27784 in 35:11:10\n",
      "Processed 20500 out of 27784 in 35:21:14\n",
      "Processed 20550 out of 27784 in 35:36:50\n",
      "Processed 20600 out of 27784 in 37:01:00\n",
      "Processed 20650 out of 27784 in 37:17:15\n",
      "Processed 20700 out of 27784 in 37:34:07\n",
      "Processed 20750 out of 27784 in 37:47:27\n",
      "Processed 20800 out of 27784 in 38:18:01\n",
      "Processed 20850 out of 27784 in 38:44:18\n",
      "Processed 20900 out of 27784 in 38:45:20\n",
      "Processed 20950 out of 27784 in 38:47:45\n",
      "Processed 21000 out of 27784 in 38:51:38\n",
      "Processed 21050 out of 27784 in 38:53:52\n",
      "Processed 21100 out of 27784 in 38:55:45\n",
      "Processed 21150 out of 27784 in 38:56:44\n",
      "Processed 21200 out of 27784 in 38:58:44\n",
      "Processed 21250 out of 27784 in 38:59:57\n",
      "Processed 21300 out of 27784 in 39:01:31\n",
      "Processed 21350 out of 27784 in 39:03:20\n",
      "Processed 21400 out of 27784 in 39:06:25\n",
      "Processed 21450 out of 27784 in 39:08:28\n",
      "Processed 21500 out of 27784 in 39:13:41\n",
      "Processed 21550 out of 27784 in 39:14:45\n",
      "Processed 21600 out of 27784 in 39:15:04\n",
      "Processed 21650 out of 27784 in 39:15:45\n",
      "Processed 21700 out of 27784 in 39:17:39\n",
      "Processed 21750 out of 27784 in 39:19:40\n",
      "Processed 21800 out of 27784 in 39:22:11\n",
      "Processed 21850 out of 27784 in 39:24:47\n",
      "Processed 21900 out of 27784 in 39:26:38\n",
      "Processed 21950 out of 27784 in 39:27:55\n",
      "Processed 22000 out of 27784 in 39:29:14\n",
      "Processed 22050 out of 27784 in 39:30:31\n",
      "Processed 22100 out of 27784 in 39:32:37\n",
      "Processed 22150 out of 27784 in 39:34:55\n",
      "Processed 22200 out of 27784 in 39:38:06\n",
      "Processed 22250 out of 27784 in 39:41:01\n",
      "Processed 22300 out of 27784 in 39:42:49\n",
      "Processed 22350 out of 27784 in 39:45:13\n",
      "Processed 22400 out of 27784 in 39:49:19\n",
      "Processed 22450 out of 27784 in 39:52:29\n",
      "Processed 22500 out of 27784 in 39:55:51\n",
      "Processed 22550 out of 27784 in 39:58:27\n",
      "Processed 22600 out of 27784 in 39:59:18\n",
      "Processed 22650 out of 27784 in 40:01:28\n",
      "Processed 22700 out of 27784 in 40:04:44\n",
      "Processed 22750 out of 27784 in 40:08:04\n",
      "Processed 22800 out of 27784 in 40:10:09\n",
      "Processed 22850 out of 27784 in 40:11:12\n",
      "Processed 22900 out of 27784 in 40:13:05\n",
      "Processed 22950 out of 27784 in 40:15:00\n",
      "Processed 23000 out of 27784 in 40:19:29\n",
      "Processed 23050 out of 27784 in 40:21:50\n",
      "Processed 23100 out of 27784 in 40:24:25\n",
      "Processed 23150 out of 27784 in 40:29:37\n",
      "Processed 23200 out of 27784 in 40:32:45\n",
      "Processed 23250 out of 27784 in 40:35:43\n",
      "Processed 23300 out of 27784 in 40:37:24\n",
      "Processed 23350 out of 27784 in 40:38:41\n",
      "Processed 23400 out of 27784 in 40:39:51\n",
      "Processed 23450 out of 27784 in 40:40:56\n",
      "Processed 23500 out of 27784 in 40:42:16\n",
      "Processed 23550 out of 27784 in 40:43:11\n",
      "Processed 23600 out of 27784 in 40:44:31\n",
      "Processed 23650 out of 27784 in 40:45:23\n",
      "Processed 23700 out of 27784 in 40:46:40\n",
      "Processed 23750 out of 27784 in 40:47:34\n",
      "Processed 23800 out of 27784 in 40:48:35\n",
      "Processed 23850 out of 27784 in 40:53:28\n",
      "Processed 23900 out of 27784 in 40:55:09\n",
      "Processed 23950 out of 27784 in 40:56:08\n",
      "Processed 24000 out of 27784 in 40:58:36\n",
      "Processed 24050 out of 27784 in 41:00:58\n",
      "Processed 24100 out of 27784 in 41:03:29\n",
      "Processed 24150 out of 27784 in 41:06:06\n",
      "Processed 24200 out of 27784 in 41:08:59\n",
      "Processed 24250 out of 27784 in 41:11:11\n",
      "Processed 24300 out of 27784 in 41:13:59\n",
      "Processed 24350 out of 27784 in 41:14:53\n",
      "Processed 24400 out of 27784 in 41:19:09\n",
      "Processed 24450 out of 27784 in 41:22:13\n",
      "Processed 24500 out of 27784 in 41:23:49\n",
      "Processed 24550 out of 27784 in 41:25:08\n",
      "Processed 24600 out of 27784 in 41:26:49\n",
      "Processed 24650 out of 27784 in 41:28:19\n",
      "Processed 24700 out of 27784 in 41:29:26\n",
      "Processed 24750 out of 27784 in 41:32:50\n",
      "Processed 24800 out of 27784 in 41:36:25\n",
      "Processed 24850 out of 27784 in 41:37:18\n",
      "Processed 24900 out of 27784 in 41:38:24\n",
      "Processed 24950 out of 27784 in 41:39:23\n",
      "Processed 25000 out of 27784 in 41:41:12\n",
      "Processed 25050 out of 27784 in 41:42:45\n",
      "Processed 25100 out of 27784 in 41:48:04\n",
      "Processed 25150 out of 27784 in 42:03:31\n",
      "Processed 25200 out of 27784 in 42:10:45\n",
      "Processed 25250 out of 27784 in 42:22:33\n",
      "Processed 25300 out of 27784 in 42:41:15\n",
      "Processed 25350 out of 27784 in 42:56:33\n",
      "Processed 25400 out of 27784 in 43:04:23\n",
      "Processed 25450 out of 27784 in 43:17:14\n",
      "Processed 25500 out of 27784 in 43:30:54\n",
      "Processed 25550 out of 27784 in 43:40:48\n",
      "Processed 25600 out of 27784 in 43:47:28\n",
      "Processed 25650 out of 27784 in 43:58:46\n",
      "Processed 25700 out of 27784 in 44:09:41\n",
      "Processed 25750 out of 27784 in 44:19:57\n",
      "Processed 25800 out of 27784 in 44:29:41\n",
      "Processed 25850 out of 27784 in 44:36:11\n",
      "Processed 25900 out of 27784 in 44:46:57\n",
      "Processed 25950 out of 27784 in 45:02:04\n",
      "Processed 26000 out of 27784 in 45:09:04\n",
      "Processed 26050 out of 27784 in 45:18:20\n",
      "Processed 26100 out of 27784 in 45:59:44\n",
      "Processed 26150 out of 27784 in 46:15:06\n",
      "Processed 26200 out of 27784 in 46:28:01\n",
      "Processed 26250 out of 27784 in 46:44:36\n",
      "Processed 26300 out of 27784 in 47:02:42\n",
      "Processed 26350 out of 27784 in 47:17:23\n",
      "Processed 26400 out of 27784 in 47:32:00\n",
      "Processed 26450 out of 27784 in 47:41:42\n",
      "Processed 26500 out of 27784 in 47:57:40\n",
      "Processed 26550 out of 27784 in 48:06:41\n",
      "Processed 26600 out of 27784 in 48:23:24\n",
      "Processed 26650 out of 27784 in 48:34:54\n",
      "Processed 26700 out of 27784 in 48:42:10\n",
      "Processed 26750 out of 27784 in 48:49:09\n",
      "Processed 26800 out of 27784 in 49:00:30\n",
      "Processed 26850 out of 27784 in 49:11:57\n",
      "Processed 26900 out of 27784 in 49:30:32\n",
      "Processed 26950 out of 27784 in 49:42:43\n",
      "Processed 27000 out of 27784 in 49:52:12\n",
      "Processed 27050 out of 27784 in 50:03:04\n",
      "Processed 27100 out of 27784 in 50:12:28\n",
      "Processed 27150 out of 27784 in 50:28:57\n",
      "Processed 27200 out of 27784 in 50:46:06\n",
      "Processed 27250 out of 27784 in 50:53:31\n",
      "Processed 27300 out of 27784 in 51:06:34\n",
      "Processed 27350 out of 27784 in 51:13:36\n",
      "Processed 27400 out of 27784 in 51:29:41\n",
      "Processed 27450 out of 27784 in 51:38:21\n",
      "Processed 27500 out of 27784 in 51:54:26\n",
      "Processed 27550 out of 27784 in 52:11:23\n",
      "Processed 27600 out of 27784 in 52:26:05\n",
      "Processed 27650 out of 27784 in 52:48:05\n",
      "Processed 27700 out of 27784 in 52:57:38\n",
      "Processed 27750 out of 27784 in 53:08:11\n",
      "Processed 27784 out of 27784 in 53:14:48\n"
     ]
    }
   ],
   "source": [
    "def format_time(seconds):\n",
    "    hours, remainder = divmod(int(seconds), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "start_time = time.time()\n",
    "# form the language classification file\n",
    "files_processed = 0\n",
    "files_to_process = len(fd)\n",
    "while files_to_process > files_processed:\n",
    "    processed_files = process_record_batch(fd, batch_size=50)\n",
    "    files_processed = len(processed_files)\n",
    "    print(f'Processed {files_processed} out of {files_to_process} in {format_time(time.time()-start_time)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyAudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2abc5501ad3d8eff2da7865cf45fd15699605a857f48109c31373cdc3963a944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
