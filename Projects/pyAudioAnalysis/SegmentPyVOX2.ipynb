{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment GRN data using pyAudioAnalysis\n",
    "\n",
    "The group of notebooks SegmentPyVOXn.ipynb (n=0-9) will be used to segment the data using the segmentation algorithm I developed in DevelopASSegmentation.ipynb. It follows the same pattern as used in SegmentFVOXn.ipynb.\n",
    "\n",
    "# Data to be Created\n",
    "\n",
    "Three sets of data will be made, 4, 6, and 10 second data. \n",
    "\n",
    "This file will take a long time to run. For that reason it will check point its progress at regular intervals by writing smaller files. If its progress is restarted for any reason it will use those files to determine where it was up to. Segmenting the data is the slow part. For each audio file a segment file will be generated. If mp3 files are to be generated later using different parameters the segmented files should make the process much quicker.\n",
    "\n",
    "The input to this process is the same as SegmentFVOX.ipynb. We actually use its division of the files into 10 groups.\n",
    "\n",
    "The output will be:\n",
    "    \n",
    "    1. /media/originals/py_audio_seg/[iso]/[filename].pkl\n",
    "        where each pkl file contains the list of raw segments for the item.\n",
    "    2. /media/originals/py_audio_seg/seg_4sec.csv\n",
    "        all the metadata for the seg_4sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    3. /media/originals/py_audio_seg/seg_6sec.csv\n",
    "        all the metadata for the seg_6sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    4. /media/originals/py_audio_seg/seg_10sec.csv\n",
    "        all the metadata for the seg_10sec dataset. Metadata needed for the dataset can be derived from this.\n",
    "    5. /media/originals/datasets/py_audio_seg_4sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n",
    "    6. /media/originals/datasets/py_audio_seg_6sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n",
    "    7. /media/originals/datasets/py_audio_seg_10sec/data/[iso]/[program_id]_[item_no]_[seg].mp3\n",
    "        which is an mp3 for each segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('~/work/pyAudioAnalysis'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from collections import namedtuple\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "from pyAudioAnalysis import audioTrainTest as at\n",
    "from pyAudioAnalysis import MidTermFeatures as mtf\n",
    "from pyAudioAnalysis import audioBasicIO as aIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the locations for each of the file types\n",
    "NOTEBOOK_ID=2\n",
    "SEGMENTS_DIR = '/media/originals/py_audio_seg/'\n",
    "DATASETS_DIR = '/media/originals/datasets/'\n",
    "SEC_4_DATA_DIR = 'py_audio_seg_4sec/data/'\n",
    "SEC_6_DATA_DIR = 'py_audio_seg_6sec/data/'\n",
    "SEC_10_DATA_DIR = 'py_audio_seg_10sec/data/'\n",
    "\n",
    "# define specific files used in the process\n",
    "SEG_4_SEC_DF = f'{SEGMENTS_DIR}seg_4sec_{NOTEBOOK_ID}.csv'\n",
    "SEG_6_SEC_DF = f'{SEGMENTS_DIR}seg_6sec_{NOTEBOOK_ID}.csv'\n",
    "SEG_10_SEC_DF = f'{SEGMENTS_DIR}seg_10sec_{NOTEBOOK_ID}.csv'\n",
    "\n",
    "# define segment sizes for each dataset\n",
    "SEG_4_SEC = 4.0\n",
    "SEG_6_SEC = 6.0\n",
    "SEG_10_SEC = 10.0\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "def convert_to_ms(sec):\n",
    "    return int(sec*1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'iso', 'language_name', 'track', 'location', 'year',\n",
       "       'path', 'filename', 'length', 'program'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now read in the description of the input and remove the unwanted columns and rename the rest to be python attribute names.\n",
    "fd = pd.read_csv(f'/media/originals/fsegs/files_{NOTEBOOK_ID}.csv')\n",
    "fd.set_index('ID', inplace=True)\n",
    "fd.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                   16258\n",
      "iso                                                            duu\n",
      "language_name                                                Drung\n",
      "track                                                           11\n",
      "location                                                   Nujiang\n",
      "year                                                        2001.0\n",
      "path                             Programs/31/31860/A31860/From_CM/\n",
      "filename         C31860B Region 00_06_16_234 to 00_07_24_757 (0...\n",
      "length                                                   68.522667\n",
      "program                                                      31860\n",
      "Name: 31860_011, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fd.iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['64418_019', '64418_030', '64418_017', '64418_016', '64418_031',\n",
      "       '64418_011', '64418_032', '64418_034', '64418_013', '64418_014',\n",
      "       ...\n",
      "       '74542_002', '74541_021', '74541_020', '74541_019', '10810_001',\n",
      "       '10810_002', '74541_031', '74541_035', '74541_034', '74541_018'],\n",
      "      dtype='object', name='ID', length=20837)\n"
     ]
    }
   ],
   "source": [
    "print(fd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate directories and filenames\n",
    "def prepare_dir(dirname):\n",
    "    if dirname[-1] != '/':\n",
    "        dirname = dirname + '/'\n",
    "    Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "    return dirname\n",
    "\n",
    "def prepare_raw_seg_dir(fd):\n",
    "    return prepare_dir(SEGMENTS_DIR + fd.iso)\n",
    "\n",
    "def raw_seg_filename(fd):\n",
    "    return f'{fd.filename}.pkl'\n",
    "\n",
    "def prepare_dataset_data_dir(fd, dataset_dir):\n",
    "    return prepare_dir(DATASETS_DIR + dataset_dir + fd.iso)\n",
    "\n",
    "def seg_mp3_filename(fd, seg):\n",
    "    return f'{fd.filename[:-4]}_{seg:03d}.mp3'\n",
    "\n",
    "def get_fname(fd):\n",
    "    path = fd.path\n",
    "    if path[-1] != '/':\n",
    "        path = path + '/'\n",
    "    files = glob.glob('/media/programs/' + path + fd.filename.replace('\\ufffd', '*'))\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    return '/media/programs/' + path + fd.filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def condition_audio_segment(audio_seg):\n",
    "    if audio_seg.channels != 1:\n",
    "        audio_seg = audio_seg.set_channels(1)\n",
    "\n",
    "    if audio_seg.sample_width != 2:\n",
    "        audio_seg = audio_seg.set_sample_width(2)\n",
    "\n",
    "    if audio_seg.frame_rate != SAMPLING_RATE:\n",
    "        audio_seg = audio_seg.set_frame_rate(SAMPLING_RATE)\n",
    "    return audio_seg\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# sklearn puts out a lot of annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Segment = namedtuple('Segment', ['start', 'end', 'classification'])\n",
    "\n",
    "# re-implement a simplified version of mid_term_file_classification\n",
    "# it is implemented as a class to allow the model to be cached.\n",
    "class ExtractVoiceSegments():\n",
    "    classifier, mean, std, class_names, mt_win, mid_step, st_win, \\\n",
    "         st_step, compute_beat = at.load_model('/home/jovyan/work/pyAudioAnalysis/pyAudioAnalysis/data/models/svm_rbf_4class')\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def segments_in(self, signal, sampling_rate, offset):\n",
    "        labels = []\n",
    "\n",
    "        # mid-term feature extraction:\n",
    "        mt_feats, _, _ = \\\n",
    "            mtf.mid_feature_extraction(signal, sampling_rate,\n",
    "                                    ExtractVoiceSegments.mt_win * sampling_rate,\n",
    "                                    ExtractVoiceSegments.mid_step * sampling_rate,\n",
    "                                    round(sampling_rate * ExtractVoiceSegments.st_win),\n",
    "                                    round(sampling_rate * ExtractVoiceSegments.st_step))\n",
    "\n",
    "        # for each feature vector (i.e. for each fix-sized segment):\n",
    "        for col_index in range(mt_feats.shape[1]):\n",
    "            # normalize current feature v\n",
    "            feature_vector = (mt_feats[:, col_index] - ExtractVoiceSegments.mean) / ExtractVoiceSegments.std\n",
    "\n",
    "            # classify vector:\n",
    "            label_predicted, _ = \\\n",
    "                at.classifier_wrapper(ExtractVoiceSegments.classifier, 'svm', feature_vector)\n",
    "            labels.append(label_predicted)\n",
    "\n",
    "        segs, classes = aS.labels_to_segments(labels, ExtractVoiceSegments.mid_step)\n",
    "        # there is a bug in labels to segments when there is a single label. In this case it returns a list rather than a list of lists\n",
    "        if len(labels) == 1:\n",
    "            segs = [].append(segs)\n",
    "        return [] if segs is None else [Segment(seg[0]+offset, seg[1]+offset, ExtractVoiceSegments.class_names[int(label)]) for seg, label in zip(segs, classes)]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function performs mid-term classification of an audio stream.\n",
    "Towards this end, supervised knowledge is used,\n",
    "i.e. a pre-trained classifier.\n",
    "ARGUMENTS:\n",
    "    - input_file:        path of the input WAV/mp3 file\n",
    "RETURNS:\n",
    "    - list of Segments (see above tuple)\n",
    "\"\"\"\n",
    "def extract_voice_segments(input_file, *, __extract_voice_segments=ExtractVoiceSegments()):\n",
    "    segments = []\n",
    "\n",
    "    # load input file\n",
    "    sampling_rate, signal = aIO.read_audio_file(input_file)\n",
    "\n",
    "    # could not read file\n",
    "    if sampling_rate == 0:\n",
    "        return segments\n",
    "\n",
    "    # convert stereo (if) to mono\n",
    "    signal = aIO.stereo_to_mono(signal)\n",
    "\n",
    "    # find the silence segments\n",
    "    non_silent_segments = aS.silence_removal(signal, sampling_rate, 0.02, 0.02, smooth_window=1.0, weight=0.3)\n",
    "\n",
    "    # work through each segment\n",
    "    for seg in non_silent_segments:\n",
    "        start = int(seg[0]*sampling_rate)\n",
    "        stop = int(seg[1]*sampling_rate)\n",
    "        sig = signal[start:stop]\n",
    "\n",
    "        segments.extend(__extract_voice_segments.segments_in(signal[start:stop], sampling_rate, seg[0]))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch = namedtuple('Epoch', ['start', 'end'])\n",
    "def speech_epochs_from_segments(segments, epoch_length=4.0, silence_tolerance=0.0):\n",
    "    epochs = []\n",
    "    i = 0\n",
    "    silence_this_epoch = silence_tolerance\n",
    "    while i < len(segments):\n",
    "        seg_duration = segments[i].end - segments[i].start\n",
    "        if segments[i].classification != 'speech':\n",
    "            silence_this_epoch = silence_tolerance\n",
    "\n",
    "        elif seg_duration >= epoch_length:\n",
    "            epochs.append(Epoch(segments[i].start, segments[i].start+epoch_length))\n",
    "            silence_this_epoch = silence_tolerance\n",
    "            # process the same segment again with a smaller size\n",
    "            new_start = segments[i].start+epoch_length\n",
    "            new_end = segments[i].end\n",
    "            if new_start < new_end:\n",
    "                segments[i] = Segment(new_start, new_end, segments[i].classification)\n",
    "                continue\n",
    "        else:\n",
    "            if i+1 < len(segments):\n",
    "                if (segments[i].end + silence_this_epoch) >= segments[i+1].start and segments[i+1].classification == 'speech':\n",
    "                    # did we use up any silence tolerence\n",
    "                    if segments[i].end < segments[i+1].start:\n",
    "                        silence_this_epoch -= (segments[i+1].start - segments[i].end)\n",
    "                    segments[i+1] = Segment(segments[i].start, segments[i+1].end, segments[i].classification)\n",
    "                else:\n",
    "                    silence_this_epoch = silence_tolerance\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_the_segment_info(fd, segs):\n",
    "    fname = prepare_raw_seg_dir(fd) + raw_seg_filename(fd)\n",
    "    with open(fname, 'wb') as pklFile:\n",
    "         pkl.dump(segs, pklFile)\n",
    "\n",
    "def read_the_segment_info(fd):\n",
    "    fname = prepare_raw_seg_dir(fd) + raw_seg_filename(fd)\n",
    "    if os.path.exists(fname):\n",
    "        if os.path.getsize(fname) > 0:\n",
    "            with open(fname, 'rb') as pklFile:\n",
    "                return pkl.load(pklFile)\n",
    "    return []\n",
    "    \n",
    "def update_dataframes(seg_df_csv, seg_records):\n",
    "    # now update the dataframes\n",
    "    if len(seg_records) > 0:\n",
    "        if os.path.isfile(seg_df_csv):\n",
    "            seg_sec_df = pd.concat([pd.read_csv(seg_df_csv, index_col='file_name'), pd.DataFrame.from_records(seg_records, index='file_name')])\n",
    "        else:\n",
    "            seg_sec_df = pd.DataFrame.from_records(seg_records, index='file_name')\n",
    "        seg_sec_df.to_csv(seg_df_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_segments_for_file(fd):\n",
    "    fmt = 'wav'\n",
    "    if fd.filename[-4:].lower() == '.mp3' :\n",
    "        fmt = 'mp3'\n",
    "    audio_seg = AudioSegment.from_file(get_fname(fd), format=fmt)\n",
    "\n",
    "    # now condition the segment and extract the raw segments.\n",
    "    audio_seg = condition_audio_segment(audio_seg)\n",
    "    segs = read_the_segment_info(fd)\n",
    "    if len(segs) == 0:\n",
    "        segs = extract_voice_segments(get_fname(fd))\n",
    "        save_the_segment_info(fd, segs)\n",
    "\n",
    "    return audio_seg, segs\n",
    "\n",
    "\n",
    "def create_mp3_files(audio_seg, segs, time_per_segment, dataset_dir, fd):\n",
    "    epochs_for_time = speech_epochs_from_segments(segs, epoch_length=time_per_segment, silence_tolerance=time_per_segment/4.0)\n",
    "\n",
    "    # now write out the 4 sec segments\n",
    "    dirname = prepare_dataset_data_dir(fd, dataset_dir)\n",
    "    rows = list()\n",
    "\n",
    "    for i, seg in enumerate(epochs_for_time):\n",
    "        file_name = dataset_dir + fd.iso + '/' + seg_mp3_filename(fd, i)\n",
    "        fname = dirname + seg_mp3_filename(fd, i)\n",
    "        start = convert_to_ms(seg.start)\n",
    "        stop = convert_to_ms(seg.end)\n",
    "        if not os.path.exists(fname):\n",
    "            audio_seg[start:stop].export(fname, format='mp3', bitrate='32k')\n",
    "        desc = dict(fd._asdict())\n",
    "        desc['seg_start'] = start\n",
    "        desc['seg_stop'] = stop\n",
    "        desc['seg'] = i\n",
    "        desc['file_name'] = file_name\n",
    "        rows.append(desc)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing these items might take a very long time. To permit the process to be interrupted and restarted the indexes of processed items\n",
    "# are maintained in a set that is pickled on each batch. This allows the batch to quickly pick up where it left off.\n",
    "def process_record_batch(files_df, *, batch_size=10):\n",
    "    batch_proc = 0\n",
    "    processed_file = f'{SEGMENTS_DIR}processed16_{NOTEBOOK_ID}.pkl'\n",
    "    if os.path.isfile(processed_file):\n",
    "        with open(processed_file, 'rb') as pklFile:\n",
    "            processed_files = pkl.load(pklFile)\n",
    "    else:\n",
    "        processed_files = set()\n",
    "\n",
    "    segmented_4sec_segs = []\n",
    "    segmented_6sec_segs = []\n",
    "    segmented_10sec_segs = []\n",
    "\n",
    "    for fd in files_df.itertuples():\n",
    "        if batch_proc < batch_size:\n",
    "            if fd.Index not in processed_files:\n",
    "                try:\n",
    "                    audio_seg, voice_segs = extract_audio_segments_for_file(fd)\n",
    "\n",
    "                    segmented_4sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_4_SEC, SEC_4_DATA_DIR, fd))\n",
    "                    segmented_6sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_6_SEC, SEC_6_DATA_DIR, fd))\n",
    "                    segmented_10sec_segs.extend(create_mp3_files(audio_seg, voice_segs, SEG_10_SEC, SEC_10_DATA_DIR, fd))\n",
    "                except:\n",
    "                    print(f'exception {fd.filename}')\n",
    "                    pass\n",
    "\n",
    "                # we want to add an fd that has an exception so it is not reprocessed on every batch\n",
    "                processed_files.add(fd.Index)\n",
    "                batch_proc += 1\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    update_dataframes(SEG_4_SEC_DF, segmented_4sec_segs)\n",
    "    update_dataframes(SEG_6_SEC_DF, segmented_6sec_segs)\n",
    "    update_dataframes(SEG_10_SEC_DF, segmented_10sec_segs)\n",
    "\n",
    "    with open(processed_file, 'wb') as pklFile:\n",
    "        pkl.dump(processed_files, pklFile)\n",
    "\n",
    "    return processed_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10550 out of 20837 in 308.3766429424286 seconds\n",
      "Processed 10600 out of 20837 in 2018.3467071056366 seconds\n",
      "Processed 10650 out of 20837 in 2878.678300142288 seconds\n",
      "Processed 10700 out of 20837 in 3495.63573718071 seconds\n",
      "Processed 10750 out of 20837 in 4227.513229846954 seconds\n",
      "Processed 10800 out of 20837 in 5662.45556306839 seconds\n",
      "Processed 10850 out of 20837 in 6567.364106655121 seconds\n",
      "Processed 10900 out of 20837 in 7838.461446523666 seconds\n",
      "Processed 10950 out of 20837 in 9488.060826778412 seconds\n",
      "Processed 11000 out of 20837 in 10515.359589576721 seconds\n",
      "Processed 11050 out of 20837 in 12338.590985536575 seconds\n",
      "Processed 11100 out of 20837 in 12778.233350992203 seconds\n",
      "Processed 11150 out of 20837 in 13054.527465581894 seconds\n",
      "Processed 11200 out of 20837 in 13357.623558044434 seconds\n",
      "Processed 11250 out of 20837 in 13949.976852416992 seconds\n",
      "Processed 11300 out of 20837 in 14737.342000484467 seconds\n",
      "Processed 11350 out of 20837 in 15591.390249490738 seconds\n",
      "Processed 11400 out of 20837 in 16366.283353328705 seconds\n",
      "Processed 11450 out of 20837 in 17127.338666200638 seconds\n",
      "Processed 11500 out of 20837 in 17716.848692178726 seconds\n",
      "Processed 11550 out of 20837 in 19336.466024160385 seconds\n",
      "Processed 11600 out of 20837 in 20420.12910938263 seconds\n",
      "Processed 11650 out of 20837 in 20785.237235546112 seconds\n",
      "Processed 11700 out of 20837 in 21243.12392091751 seconds\n",
      "Processed 11750 out of 20837 in 21906.96559524536 seconds\n",
      "Processed 11800 out of 20837 in 22445.96937084198 seconds\n",
      "Processed 11850 out of 20837 in 23454.650194883347 seconds\n",
      "Processed 11900 out of 20837 in 24182.01391506195 seconds\n",
      "Processed 11950 out of 20837 in 25450.041417360306 seconds\n",
      "Processed 12000 out of 20837 in 26636.31089949608 seconds\n",
      "Processed 12050 out of 20837 in 28226.35374069214 seconds\n",
      "Processed 12100 out of 20837 in 28820.127200126648 seconds\n",
      "Processed 12150 out of 20837 in 29598.289163589478 seconds\n",
      "Processed 12200 out of 20837 in 31178.779873132706 seconds\n",
      "Processed 12250 out of 20837 in 31550.769886493683 seconds\n",
      "Processed 12300 out of 20837 in 32060.69928240776 seconds\n",
      "Processed 12350 out of 20837 in 33450.09704327583 seconds\n",
      "Processed 12400 out of 20837 in 34936.21226263046 seconds\n",
      "Processed 12450 out of 20837 in 35901.59922361374 seconds\n",
      "Processed 12500 out of 20837 in 38494.17251777649 seconds\n",
      "Processed 12550 out of 20837 in 39515.61249375343 seconds\n",
      "Processed 12600 out of 20837 in 40115.45163202286 seconds\n",
      "Processed 12650 out of 20837 in 41571.04041552544 seconds\n",
      "Processed 12700 out of 20837 in 42338.78992366791 seconds\n",
      "Processed 12750 out of 20837 in 43334.8110268116 seconds\n",
      "Processed 12800 out of 20837 in 44024.51328969002 seconds\n",
      "Processed 12850 out of 20837 in 44977.12140417099 seconds\n",
      "Processed 12900 out of 20837 in 46053.61943888664 seconds\n",
      "Processed 12950 out of 20837 in 47685.59136009216 seconds\n",
      "Processed 13000 out of 20837 in 48367.14876842499 seconds\n",
      "Processed 13050 out of 20837 in 48983.51623034477 seconds\n",
      "Processed 13100 out of 20837 in 49534.9752869606 seconds\n",
      "Processed 13150 out of 20837 in 50098.172748565674 seconds\n",
      "Processed 13200 out of 20837 in 50745.251180410385 seconds\n",
      "Processed 13250 out of 20837 in 51326.15460538864 seconds\n",
      "Processed 13300 out of 20837 in 52466.35850095749 seconds\n",
      "Processed 13350 out of 20837 in 53353.59544587135 seconds\n",
      "Processed 13400 out of 20837 in 54138.03610563278 seconds\n",
      "Processed 13450 out of 20837 in 54708.871487140656 seconds\n",
      "Processed 13500 out of 20837 in 55778.363295316696 seconds\n",
      "Processed 13550 out of 20837 in 57018.07380056381 seconds\n",
      "Processed 13600 out of 20837 in 57648.10993051529 seconds\n",
      "Processed 13650 out of 20837 in 58174.276461839676 seconds\n",
      "Processed 13700 out of 20837 in 59041.18471765518 seconds\n",
      "Processed 13750 out of 20837 in 59668.74680542946 seconds\n",
      "Processed 13800 out of 20837 in 60280.07484412193 seconds\n",
      "Processed 13850 out of 20837 in 60668.055160045624 seconds\n",
      "Processed 13900 out of 20837 in 61585.97864985466 seconds\n",
      "Processed 13950 out of 20837 in 62074.01503324509 seconds\n",
      "Processed 14000 out of 20837 in 63810.16667890549 seconds\n",
      "Processed 14050 out of 20837 in 65929.30851840973 seconds\n",
      "Processed 14100 out of 20837 in 66969.02244615555 seconds\n",
      "Processed 14150 out of 20837 in 67528.4621424675 seconds\n",
      "Processed 14200 out of 20837 in 69872.59605836868 seconds\n",
      "Processed 14250 out of 20837 in 70333.16415810585 seconds\n",
      "Processed 14300 out of 20837 in 70628.96842837334 seconds\n",
      "Processed 14350 out of 20837 in 71501.95960617065 seconds\n",
      "Processed 14400 out of 20837 in 71773.56832194328 seconds\n",
      "Processed 14450 out of 20837 in 72426.3007016182 seconds\n",
      "Processed 14500 out of 20837 in 72757.45296430588 seconds\n",
      "Processed 14550 out of 20837 in 73278.05707740784 seconds\n",
      "Processed 14600 out of 20837 in 73975.12891840935 seconds\n",
      "Processed 14650 out of 20837 in 74300.37403655052 seconds\n",
      "Processed 14700 out of 20837 in 74749.97091150284 seconds\n",
      "Processed 14750 out of 20837 in 75933.0392985344 seconds\n",
      "Processed 14800 out of 20837 in 76740.14039683342 seconds\n",
      "Processed 14850 out of 20837 in 77068.05673670769 seconds\n",
      "Processed 14900 out of 20837 in 77695.2365705967 seconds\n",
      "Processed 14950 out of 20837 in 78497.13968348503 seconds\n",
      "Processed 15000 out of 20837 in 79410.92783546448 seconds\n",
      "Processed 15050 out of 20837 in 79797.6719546318 seconds\n",
      "Processed 15100 out of 20837 in 80813.05491280556 seconds\n",
      "Processed 15150 out of 20837 in 82941.72324109077 seconds\n",
      "Processed 15200 out of 20837 in 85559.292375803 seconds\n",
      "Processed 15250 out of 20837 in 86269.32559299469 seconds\n",
      "Processed 15300 out of 20837 in 87338.86645627022 seconds\n",
      "Processed 15350 out of 20837 in 88167.88141417503 seconds\n",
      "Processed 15400 out of 20837 in 88687.51694655418 seconds\n",
      "Processed 15450 out of 20837 in 89439.90130090714 seconds\n",
      "Processed 15500 out of 20837 in 90999.11795139313 seconds\n",
      "Processed 15550 out of 20837 in 93042.85421657562 seconds\n",
      "Processed 15600 out of 20837 in 93930.16948199272 seconds\n",
      "Processed 15650 out of 20837 in 95464.37612628937 seconds\n",
      "Processed 15700 out of 20837 in 95950.42791604996 seconds\n",
      "Processed 15750 out of 20837 in 96629.35998129845 seconds\n",
      "Processed 15800 out of 20837 in 97043.71356773376 seconds\n",
      "Processed 15850 out of 20837 in 97532.49956870079 seconds\n",
      "Processed 15900 out of 20837 in 97924.73195290565 seconds\n",
      "Processed 15950 out of 20837 in 98284.33366727829 seconds\n",
      "Processed 16000 out of 20837 in 98614.30873203278 seconds\n",
      "Processed 16050 out of 20837 in 99344.09125137329 seconds\n",
      "Processed 16100 out of 20837 in 99682.7819647789 seconds\n",
      "Processed 16150 out of 20837 in 100058.71358275414 seconds\n",
      "Processed 16200 out of 20837 in 101186.09604072571 seconds\n",
      "Processed 16250 out of 20837 in 102494.86722135544 seconds\n",
      "Processed 16300 out of 20837 in 103623.02183628082 seconds\n",
      "Processed 16350 out of 20837 in 104319.42785978317 seconds\n",
      "Processed 16400 out of 20837 in 105091.59428572655 seconds\n",
      "Processed 16450 out of 20837 in 105754.97415161133 seconds\n",
      "Processed 16500 out of 20837 in 106559.01609301567 seconds\n",
      "Processed 16550 out of 20837 in 107281.40902996063 seconds\n",
      "Processed 16600 out of 20837 in 108456.8890581131 seconds\n",
      "Processed 16650 out of 20837 in 110068.26394033432 seconds\n",
      "Processed 16700 out of 20837 in 110520.62747049332 seconds\n",
      "Processed 16750 out of 20837 in 111018.88160657883 seconds\n",
      "Processed 16800 out of 20837 in 111767.9693005085 seconds\n",
      "Processed 16850 out of 20837 in 112189.12933683395 seconds\n",
      "Processed 16900 out of 20837 in 112872.01525783539 seconds\n",
      "Processed 16950 out of 20837 in 113404.65097737312 seconds\n",
      "Processed 17000 out of 20837 in 114263.23720932007 seconds\n",
      "Processed 17050 out of 20837 in 115004.223290205 seconds\n",
      "Processed 17100 out of 20837 in 115703.25925374031 seconds\n",
      "Processed 17150 out of 20837 in 117528.38311314583 seconds\n",
      "Processed 17200 out of 20837 in 119471.06701231003 seconds\n",
      "Processed 17250 out of 20837 in 120461.42611169815 seconds\n",
      "Processed 17300 out of 20837 in 120884.79343700409 seconds\n",
      "Processed 17350 out of 20837 in 121546.0194132328 seconds\n",
      "Processed 17400 out of 20837 in 122045.47616195679 seconds\n",
      "Processed 17450 out of 20837 in 122382.36564970016 seconds\n",
      "Processed 17500 out of 20837 in 123112.47706055641 seconds\n",
      "Processed 17550 out of 20837 in 123609.52919578552 seconds\n",
      "Processed 17600 out of 20837 in 124481.53865456581 seconds\n",
      "Processed 17650 out of 20837 in 125110.06122088432 seconds\n",
      "Processed 17700 out of 20837 in 126043.43209075928 seconds\n",
      "Processed 17750 out of 20837 in 127051.57742214203 seconds\n",
      "Processed 17800 out of 20837 in 127324.62048792839 seconds\n",
      "Processed 17850 out of 20837 in 127828.61800336838 seconds\n",
      "Processed 17900 out of 20837 in 128573.04416203499 seconds\n",
      "Processed 17950 out of 20837 in 129814.78362894058 seconds\n",
      "Processed 18000 out of 20837 in 131516.64103841782 seconds\n",
      "Processed 18050 out of 20837 in 132699.8795261383 seconds\n",
      "Processed 18100 out of 20837 in 133508.4693698883 seconds\n",
      "Processed 18150 out of 20837 in 134150.4214656353 seconds\n",
      "Processed 18200 out of 20837 in 134982.50943040848 seconds\n",
      "Processed 18250 out of 20837 in 137989.64429688454 seconds\n",
      "Processed 18300 out of 20837 in 139651.9465560913 seconds\n",
      "Processed 18350 out of 20837 in 141166.0545375347 seconds\n",
      "Processed 18400 out of 20837 in 145026.02984523773 seconds\n",
      "Processed 18450 out of 20837 in 146929.17539429665 seconds\n",
      "Processed 18500 out of 20837 in 148589.06061768532 seconds\n",
      "Processed 18550 out of 20837 in 150341.56248116493 seconds\n",
      "Processed 18600 out of 20837 in 153215.4734544754 seconds\n",
      "Processed 18650 out of 20837 in 154589.55098581314 seconds\n",
      "Processed 18700 out of 20837 in 155059.86328291893 seconds\n",
      "Processed 18750 out of 20837 in 156108.14966607094 seconds\n",
      "Processed 18800 out of 20837 in 158605.37651467323 seconds\n",
      "Processed 18850 out of 20837 in 159258.01995301247 seconds\n",
      "Processed 18900 out of 20837 in 160081.57374572754 seconds\n",
      "Processed 18950 out of 20837 in 160783.01543617249 seconds\n",
      "Processed 19000 out of 20837 in 161429.79993510246 seconds\n",
      "Processed 19050 out of 20837 in 162106.5849020481 seconds\n",
      "Processed 19100 out of 20837 in 162912.65055131912 seconds\n",
      "Processed 19150 out of 20837 in 163671.4594643116 seconds\n",
      "Processed 19200 out of 20837 in 165447.68913912773 seconds\n",
      "Processed 19250 out of 20837 in 166823.1877708435 seconds\n",
      "Processed 19300 out of 20837 in 168110.014898777 seconds\n",
      "Processed 19350 out of 20837 in 168407.52688694 seconds\n",
      "Processed 19400 out of 20837 in 169705.38847708702 seconds\n",
      "Processed 19450 out of 20837 in 170807.4990143776 seconds\n",
      "Processed 19500 out of 20837 in 171325.21125602722 seconds\n",
      "Processed 19550 out of 20837 in 173541.72169232368 seconds\n",
      "Processed 19600 out of 20837 in 174522.6861064434 seconds\n",
      "Processed 19650 out of 20837 in 175192.43617153168 seconds\n",
      "Processed 19700 out of 20837 in 175639.72724795341 seconds\n",
      "Processed 19750 out of 20837 in 176292.93428230286 seconds\n",
      "Processed 19800 out of 20837 in 176761.10274124146 seconds\n",
      "Processed 19850 out of 20837 in 177283.04117965698 seconds\n",
      "Processed 19900 out of 20837 in 177741.50219106674 seconds\n",
      "Processed 19950 out of 20837 in 178201.12733340263 seconds\n",
      "Processed 20000 out of 20837 in 180150.0943057537 seconds\n",
      "Processed 20050 out of 20837 in 181071.0097374916 seconds\n",
      "Processed 20100 out of 20837 in 181750.63999128342 seconds\n",
      "Processed 20150 out of 20837 in 183511.90114164352 seconds\n",
      "Processed 20200 out of 20837 in 186798.24063777924 seconds\n",
      "Processed 20250 out of 20837 in 187417.55971097946 seconds\n",
      "Processed 20300 out of 20837 in 187980.96917510033 seconds\n",
      "Processed 20350 out of 20837 in 188742.53252530098 seconds\n",
      "Processed 20400 out of 20837 in 189242.7768087387 seconds\n",
      "Processed 20450 out of 20837 in 189953.98232126236 seconds\n",
      "Processed 20500 out of 20837 in 190514.1305820942 seconds\n",
      "Processed 20550 out of 20837 in 191385.23289084435 seconds\n",
      "Processed 20600 out of 20837 in 192108.43638515472 seconds\n",
      "Processed 20650 out of 20837 in 193002.75890517235 seconds\n",
      "Processed 20700 out of 20837 in 193539.76217794418 seconds\n",
      "Processed 20750 out of 20837 in 194095.2868592739 seconds\n",
      "Processed 20800 out of 20837 in 194821.31453084946 seconds\n",
      "Processed 20837 out of 20837 in 195597.06940436363 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# form the language classification file\n",
    "files_processed = 0\n",
    "files_to_process = len(fd)\n",
    "while files_to_process > files_processed:\n",
    "    processed_files = process_record_batch(fd, batch_size=50)\n",
    "    files_processed = len(processed_files)\n",
    "    print(f'Processed {files_processed} out of {files_to_process} in {time.time()-start_time} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyAudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2abc5501ad3d8eff2da7865cf45fd15699605a857f48109c31373cdc3963a944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
