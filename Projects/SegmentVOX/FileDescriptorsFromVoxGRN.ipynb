{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the files recovered from vox-grn into our database for processing.\n",
    "\n",
    "The successful recovery of files from vox-grn now leaves the task of integrating the files with our existing data. The major issue is that the vox-grn data does not have any information at the item granularity. This is a problem mainly because the item type is important to filter out instrumentals and singing - although singing could be a sub-group that we export.\n",
    "\n",
    "It has been noted that some information can be extracted from the filename. The file name appears to follow this pattern:\n",
    "\n",
    "> ./Audio_MP3/[nn]/[ppppp]/[Language name][Track Title][ttt][Item 1 title] ♦ [Item 2 title] [...] [ppppp].mp3\n",
    "\n",
    "where:\n",
    "\n",
    "* nn are the first two digits of the program\n",
    "* ppppp is the program number zero padded to always be 5 digits.\n",
    "* ttt is the track number zero padded to always be 3 digits.\n",
    "\n",
    "There are as many ♦ separated item titles as there are items in the track.\n",
    "\n",
    "Our aim here is to preserve as much meta-data as we can and associate files with program items. We do not filter out based on item type - that can be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data\n",
    "First read in the descriptors of vox-grn and grid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File descriptors shape: (210704, 12)\n",
      "Program items shape: (267681, 21)\n"
     ]
    }
   ],
   "source": [
    "# Now read in the description of the input and remove the unwanted columns and rename the rest to be python attribute names.\n",
    "file_descriptors = pd.read_csv(\"/prometheus/GRN/recording_files_with_tags_and_track.csv\")\n",
    "print(f'File descriptors shape: {file_descriptors.shape}')\n",
    "items = pd.read_csv(\"/prometheus/GRN/grid_program_items.csv\")\n",
    "print(f'Program items shape: {items.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using a generator\n",
    "def gen_vox_grn():\n",
    "  resp = requests.get('https://raw.githubusercontent.com/johno-source/vox-grn/main/data/vox-grn.json')\n",
    "  vox_dict = json.loads(resp.text)\n",
    "  for iso in vox_dict.keys():\n",
    "    lang_df = pd.json_normalize(vox_dict[iso])\n",
    "    lang_df['iso'] = iso\n",
    "    yield lang_df\n",
    "\n",
    "vox_df = pd.concat(gen_vox_grn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Description\n",
    "Give a description of the columns and size of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vox-grn columns: Index(['file', 'language name', 'location', 'copyright', 'year', 'disguised',\n",
      "       'length', 'iso'],\n",
      "      dtype='object')\n",
      "recording_files_with_tags_and_track columns: Index(['LanguageID', 'ISO', 'Language', 'Program', 'Track', 'Recordist',\n",
      "       'Location', 'Year', 'Path', 'Filename', 'Size', 'Length'],\n",
      "      dtype='object')\n",
      "program items columns: Index(['Program Number', 'Program Item Number', 'Tape Side', 'Track Number',\n",
      "       'Original Recording Number', 'Original Item Number', 'Title',\n",
      "       'Vernacular Item Title', 'Language Number', 'Language Name',\n",
      "       'Item Start Time', 'Item Time', 'Finish Time', 'Original Time',\n",
      "       'Script Number', 'Script Name', 'Picture Number', 'Item Type',\n",
      "       'Comments', 'Entered By', 'Enter On Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'vox-grn columns: {vox_df.columns}')\n",
    "print(f'recording_files_with_tags_and_track columns: {file_descriptors.columns}')\n",
    "print(f'program items columns: {items.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Identity\n",
    "\n",
    "To tie all the items together we need to give them a common identity. The best thing to use is the program and the track. We use these to create an ID with the format [ppppp]_[ttt]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the program ID and the track from each data source\n",
    "vox_df['program'] = vox_df['file'].str.extract('\\./Audio_MP3/[0-9]{2}/([0-9]{5})')\n",
    "vox_df['track'] = vox_df['file'].str.extract('\\./Audio_MP3/[0-9]{2}/[0-9]{5}/.*?([0-9]{3})')\n",
    "items['program'] = items['Program Number'].str[1:]\n",
    "file_descriptors['prog'] = file_descriptors['Program'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_df['ID'] = vox_df['program'] + '_' + vox_df['track']\n",
    "items['ID'] = items['program'] + '_' + items['Track Number'].astype(int).apply('{:0>3d}'.format)\n",
    "file_descriptors['ID'] = file_descriptors['prog'] + '_' + file_descriptors['Track'].astype(int).apply('{:0>3d}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Files\n",
    "Recreate the list of files that were downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files verified as existing for 203879 out of 210704 records.\n"
     ]
    }
   ],
   "source": [
    "def check_for_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['Path'] + item_row['Filename'] )\n",
    "\n",
    "file_descriptors['file exists'] = file_descriptors.apply(check_for_file, axis=1)\n",
    "print(f'Files verified as existing for {sum(file_descriptors[\"file exists\"])} out of {file_descriptors.shape[0]} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files = file_descriptors[file_descriptors['file exists'] == False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6604 out of 6825 missing files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_files['found_prog'] = missing_files['prog'].isin(vox_df['program'])\n",
    "found_files = missing_files[missing_files['found_prog']].copy()\n",
    "print(f'Found {len(found_files)} out of {len(missing_files)} missing files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovered Files\n",
    "\n",
    "We actually downloaded any file that had a program id that was contained in the missing files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovery files: 7015\n"
     ]
    }
   ],
   "source": [
    "vox_df['recovery_candidate'] = vox_df['program'].isin(missing_files['prog'])\n",
    "recovered_files = vox_df[vox_df['recovery_candidate']].copy()\n",
    "print(f'Recovery files: {len(recovered_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put the path and file name into separate fields for merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vox_grn_key(vox_grn_dir):\n",
    "    return vox_grn_dir[2:]\n",
    "\n",
    "def extract_vox_path(vox_grn_dir):\n",
    "    vox_path_list = extract_vox_grn_key(vox_grn_dir).split('/')\n",
    "    vox_path = '.'\n",
    "    if len(vox_path_list) > 1:\n",
    "        vox_path = 'vox_grn/' + '/'.join(vox_path_list[:-1])\n",
    "    return vox_path\n",
    "\n",
    "def extract_vox_file(vox_grn_dir):\n",
    "    vox_path_list = extract_vox_grn_key(vox_grn_dir).split('/')\n",
    "    return vox_path_list[-1] if len(vox_path_list) > 1 else 'temp.mp3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_files['path'] = recovered_files['file'].apply(extract_vox_path)\n",
    "recovered_files['filename'] = recovered_files['file'].apply(extract_vox_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files with bad filenames\n",
    "It has been found that a number of files have bad filenames, including characters that cannot be used and marked as replacement character in pandas dataframes.\n",
    "\n",
    "Lets recover those and see what the overlap is with our missing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "def check_for_glob_file(item_row):\n",
    "    path_to_file = '/media/programs/' + item_row['Path'] + item_row['Filename'].replace('\\ufffd', '*')\n",
    "    files = glob.glob(path_to_file)\n",
    "    return len(files) == 1    \n",
    "\n",
    "file_descriptors['glob file exist'] = file_descriptors.apply(check_for_glob_file, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad filename in 557 files and 546 are in vox_grn.\n"
     ]
    }
   ],
   "source": [
    "file_descriptors['bad filename'] = file_descriptors['glob file exist'] & ~file_descriptors['file exists']\n",
    "bad_fd = file_descriptors[file_descriptors['bad filename']].copy()\n",
    "bad_fd['recovered'] = bad_fd['ID'].isin(recovered_files['ID'])\n",
    "print(f'Bad filename in {len(bad_fd)} files and {sum(bad_fd[\"recovered\"])} are in vox_grn.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 11 files? Are they the NaN ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_fd_unrecovered = bad_fd[bad_fd['recovered'] == False].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just check if all vox_grn files can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered files 7015 out of 7015\n"
     ]
    }
   ],
   "source": [
    "def check_for_vox_grn_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['path'] + '/' + item_row['filename'] )\n",
    "\n",
    "recovered_files['exists'] = recovered_files.apply(check_for_vox_grn_file, axis=1)\n",
    "print(f'Recovered files {sum(recovered_files[\"exists\"])} out of {len(recovered_files)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "Make sure that the files downloaded match the items we think we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate IDs in the recovered files.\n"
     ]
    }
   ],
   "source": [
    "# check that the found files have unique ids\n",
    "print(f'There are {sum(recovered_files[\"ID\"].duplicated())} duplicate IDs in the recovered files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 6604 files we believed we actually found 6598 files.\n"
     ]
    }
   ],
   "source": [
    "# Now check that there is a file for each of the missing items.\n",
    "found_files['found'] = found_files['ID'].isin(recovered_files[\"ID\"])\n",
    "print(f'Of the {len(found_files)} files we believed we actually found {sum(found_files[\"found\"])} files.')\n",
    "not_found = found_files[found_files['found'] == False]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge\n",
    "Now put each of the data sources into a state where they can be merged together.\n",
    "\n",
    "What fields do I want? And what names?\n",
    "\n",
    "grnvox_test had:\n",
    "> \"file\", \"audio\", \"iso\", \"program\", \"location\", \"item_no\", \"title\", \"item_start\", \"item_end\", \"seg_start\", \"seg_end\", \"seg\"\n",
    "\n",
    "I would like to add \"item_type\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the iso code matches for the matched items\n",
    "merge_candidates = found_files[found_files['found']].copy()\n",
    "merged_records = pd.merge(merge_candidates, recovered_files, on=\"ID\", how='inner', validate='1:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LanguageID', 'ISO', 'Language', 'Program', 'Track', 'Recordist',\n",
      "       'Location', 'Year', 'Path', 'Filename', 'Size', 'Length', 'prog', 'ID',\n",
      "       'file exists', 'found_prog', 'found', 'file', 'language name',\n",
      "       'location', 'copyright', 'year', 'disguised', 'length', 'iso',\n",
      "       'program', 'track', 'recovery_candidate', 'path', 'filename', 'exists'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_records.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the merge. Is the ISO the same? How many files are disguised? Is the length the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ISO code is the same for 6552 out of 6598 records.\n",
      "The number of files with the voice disguised is 20\n",
      "The number of files where the length difference is signficant is 11\n"
     ]
    }
   ],
   "source": [
    "print(f'The ISO code is the same for {sum(merged_records[\"ISO\"] == merged_records[\"iso\"])} out of {len(merged_records)} records.')\n",
    "print(f'The number of files with the voice disguised is {sum(merged_records[\"disguised\"] == True)}')\n",
    "print(f'The number of files where the length difference is signficant is {sum(abs(merged_records[\"Length\"] - merged_records[\"length\"]) > 1.0)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - Lets see why they are different.\n",
    "First ISO records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kvw mapped to ['aol']\n",
      "wer mapped to ['kij']\n",
      "sop mapped to ['yom']\n",
      "ppo mapped to ['hmo']\n",
      "lak mapped to ['ksp']\n",
      "ksh mapped to ['bqv']\n",
      "kxl mapped to ['kru']\n",
      "spa mapped to ['tzo', 'mxb', 'xtn', 'xtm', 'mvg', 'mil', 'zaq', 'zar', 'zad', 'zac', 'zab', 'zpt', 'zps', 'ztg', 'zpe', 'ncl', 'nhi', 'tpx', 'tcf', 'maq', 'neq', 'mco', 'ott', 'otm', 'ote', 'ots', 'otq', 'toc']\n"
     ]
    }
   ],
   "source": [
    "iso_different = merged_records[merged_records['ISO'] != merged_records['iso']]\n",
    "mapping = {}\n",
    "for rec in iso_different.itertuples():\n",
    "    if rec.ISO in mapping:\n",
    "        if rec.iso not in mapping[rec.ISO]:\n",
    "            mapping[rec.ISO].append(rec.iso)\n",
    "    else:\n",
    "        mapping[rec.ISO] = list()\n",
    "        mapping[rec.ISO].append(rec.iso)\n",
    "\n",
    "for k, v in mapping.items():\n",
    "    print(f'{k} mapped to {v}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is the language the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Language name is the same for 6404 out of 6598 records.\n",
      "The Language name is the same for 6 out of 46 records where the iso was different.\n",
      "The Location is the same for 5601 out of 6598 records.\n",
      "The Location is the same for 12 out of 46 records where the iso was different.\n"
     ]
    }
   ],
   "source": [
    "print(f'The Language name is the same for {sum(merged_records[\"Language\"] == merged_records[\"language name\"])} out of {len(merged_records)} records.')\n",
    "print(f'The Language name is the same for {sum(iso_different[\"Language\"] == iso_different[\"language name\"])} out of {len(iso_different)} records where the iso was different.')\n",
    "print(f'The Location is the same for {sum(merged_records[\"Location\"] == merged_records[\"location\"])} out of {len(merged_records)} records.')\n",
    "print(f'The Location is the same for {sum(iso_different[\"Location\"] == iso_different[\"location\"])} out of {len(iso_different)} records where the iso was different.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to say that the language given to grn-vox is more specific than the one in programs.\n",
    "\n",
    "Now lets look at the length of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the records where the locations differ\n",
    "location_diff = merged_records[merged_records[\"Location\"] != merged_records[\"location\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences were only where the location was NaN or capitalisation was different or accents were on some letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_records['length_diff'] = merged_records.Length - merged_records.length\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so none of the lengths are zero and only a couple differ significantly - take the vox-grn data for length."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set things up so that the file descriptors of the files we previously used are ready to have the newly found files appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_descriptors = file_descriptors[file_descriptors['file exists']].copy()\n",
    "original_descriptors.drop(columns=['LanguageID', 'Recordist', 'Size', 'file exists', 'Program', 'glob file exist', 'bad filename'], inplace=True)\n",
    "rename_map = { 'ISO' : 'iso', 'Language' : 'language_name', 'prog' : 'program', 'Year' : 'year',\n",
    "               'Track' : 'track', 'Location' : 'location', 'Path' : 'path', 'Filename' : 'filename', 'Length' : 'length' }\n",
    "original_descriptors.rename(columns=rename_map, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LanguageID', 'ISO', 'Language', 'Program', 'Track', 'Recordist',\n",
      "       'Location', 'Year', 'Path', 'Filename', 'Size', 'Length', 'prog', 'ID',\n",
      "       'file exists', 'found_prog', 'found', 'file', 'language name',\n",
      "       'location', 'copyright', 'year', 'disguised', 'length', 'iso',\n",
      "       'program', 'track', 'recovery_candidate', 'path', 'filename', 'exists',\n",
      "       'length_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_records.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_descriptors = merged_records.drop(columns=['LanguageID', 'ISO', 'Language', \n",
    "     'Program', 'Track', 'Recordist', 'Location', 'Year', 'Path', 'Filename', 'Size', 'Length', \n",
    "     'prog', 'file exists', 'found_prog', 'found', 'file', 'copyright', 'disguised',\n",
    "     'recovery_candidate', 'length_diff', 'exists'])\n",
    "new_descriptors.rename(columns={ 'language name' : 'language_name' }, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_fname = bad_fd_unrecovered.drop(columns=['LanguageID', 'Recordist', 'Size', 'file exists', 'Program', 'glob file exist', 'bad filename', 'recovered'])\n",
    "rename_map = { 'ISO' : 'iso', 'Language' : 'language_name', 'prog' : 'program', 'Year' : 'year',\n",
    "               'Track' : 'track', 'Location' : 'location', 'Path' : 'path', 'Filename' : 'filename', 'Length' : 'length' }\n",
    "bad_fname.rename(columns=rename_map, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'program', 'ID'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'language_name', 'location', 'year', 'length', 'iso', 'program',\n",
      "       'track', 'path', 'filename'],\n",
      "      dtype='object')\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'program', 'ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(original_descriptors.columns)\n",
    "print(new_descriptors.columns)\n",
    "print(bad_fname.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = pd.concat([original_descriptors, new_descriptors, bad_fname], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the file\n",
    "Now that we have formed a file we can save it away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of items with records are:\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'program', 'ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "fd.to_csv(\"../../data/records_with_voxgrn_files.csv\")\n",
    "print(f'The columns of items with records are:\\n{fd.columns}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate program items with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Program Number', 'Program Item Number', 'Tape Side', 'Track Number',\n",
      "       'Original Recording Number', 'Original Item Number', 'Title',\n",
      "       'Vernacular Item Title', 'Language Number', 'Language Name',\n",
      "       'Item Start Time', 'Item Time', 'Finish Time', 'Original Time',\n",
      "       'Script Number', 'Script Name', 'Picture Number', 'Item Type',\n",
      "       'Comments', 'Entered By', 'Enter On Date', 'program', 'ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(items.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 261457 usable items out of 267681 total items.\n"
     ]
    }
   ],
   "source": [
    "def usable_types(item_row):\n",
    "    unusable_items = ['Instrumental', 'Sound Effect', 'Announcement', 'Bridge']\n",
    "    return item_row['Item Type'] not in unusable_items\n",
    "\n",
    "items['usable'] = items.apply(usable_types, axis=1)\n",
    "usable_items = items[items[\"usable\"]].copy()\n",
    "\n",
    "usable_items.drop(['usable'], inplace=True, axis=1)\n",
    "print(f'There are {usable_items.shape[0]} usable items out of {items.shape[0]} total items.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 261457 records 248682 were associated with records\n"
     ]
    }
   ],
   "source": [
    "items_with_records = pd.merge(fd, usable_items, on=\"ID\", how='inner', validate='1:m')\n",
    "print(f'Of {usable_items.shape[0]} records {items_with_records.shape[0]} were associated with records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of items with records are:\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'program_x', 'ID', 'Program Number', 'Program Item Number',\n",
      "       'Tape Side', 'Track Number', 'Original Recording Number',\n",
      "       'Original Item Number', 'Title', 'Vernacular Item Title',\n",
      "       'Language Number', 'Language Name', 'Item Start Time', 'Item Time',\n",
      "       'Finish Time', 'Original Time', 'Script Number', 'Script Name',\n",
      "       'Picture Number', 'Item Type', 'Comments', 'Entered By',\n",
      "       'Enter On Date', 'program_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'The columns of items with records are:\\n{items_with_records.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that we do not want\n",
    "items_with_records.drop(columns=['Program Number', 'Tape Side', 'Track Number', 'Original Recording Number', 'Original Item Number', \n",
    "                                 'Vernacular Item Title', 'Language Number', 'Script Number', 'Script Name', 'Picture Number', 'Entered By', \n",
    "                                 'Enter On Date'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more consistency checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program differs in 0 records.\n"
     ]
    }
   ],
   "source": [
    "print(f'The program differs in {sum(items_with_records.program_x != items_with_records.program_y)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_with_records.drop(columns=['program_x', 'Comments', 'Language Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of items with records are:\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'ID', 'Program Item Number', 'Title', 'Item Start Time',\n",
      "       'Item Time', 'Finish Time', 'Original Time', 'Item Type', 'program_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'The columns of items with records are:\\n{items_with_records.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_with_records.rename(columns={'Program Item Number': 'item', 'Title': 'title', 'Item Start Time': 'start',\n",
    "        'Item Time': 'duration', 'Finish Time': 'end', 'Item Type': 'type', 'program_y' : 'program'}, inplace=True)\n",
    "items_with_records.drop(columns=['Original Time'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of items with records are:\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'ID', 'item', 'title', 'start', 'duration', 'end', 'type',\n",
      "       'program'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "items_with_records.to_csv(\"../../data/items_with_records_with_voxgrn_files.csv\")\n",
    "print(f'The columns of items with records are:\\n{items_with_records.columns}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyAudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2abc5501ad3d8eff2da7865cf45fd15699605a857f48109c31373cdc3963a944"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
