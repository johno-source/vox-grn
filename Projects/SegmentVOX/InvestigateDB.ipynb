{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation\n",
    "I wonder where the other languages have gone? In the original items there were how many languages? Why do some languages have so few segments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Languages\n",
    "First lets answer the question of where have all the languages gone.\n",
    "\n",
    "How many languages were there in the original input file? 3267 according to the analysis below. This is considerably less than the number of languages we believe are in the data set (~4500). Where have the other languages gone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['iso', 'program', 'location', 'year', 'path', 'filename', 'item_no',\n",
       "       'title', 'start', 'end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now read in the description of the input and remove the unwanted columns and rename the rest to be python attribute names.\n",
    "orig_items = pd.read_csv('/home/jovyan/work/GRN-Notebooks/Data/all_items_for_processing.csv')\n",
    "orig_items = orig_items.drop(columns=['Unnamed: 0', 'LanguageID', 'Language', 'Track', 'Recordist', 'Size', 'Length', 'Tape Side', 'Item Type', 'composite'])\n",
    "orig_items.rename(inplace=True, columns={ 'ISO' : 'iso', 'Location' : 'location', 'Year' : 'year', 'Path' : 'path', 'Filename' : 'filename', 'Title' : 'title', 'Program Item Number' : 'item_no', 'Program' : 'program' })\n",
    "\n",
    "# the existing ID is based on track number rather than item number. We want to use the item number.\n",
    "orig_items['ID'] = orig_items['program'] + '_' + orig_items['item_no'].astype(int).apply('{:0>3d}'.format)\n",
    "\n",
    "orig_items.set_index('ID', inplace=True)\n",
    "orig_items.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many languages in this set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3267"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs1 = orig_items['iso'].value_counts()\n",
    "len(langs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there were only 3267 languages in the original set. Is this because some languages did not have there item type labelled?\n",
    "\n",
    "Go back to the way the data was formed and see what is happening to the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File descriptors shape: (210704, 12)\n",
      "Program items shape: (267681, 21)\n"
     ]
    }
   ],
   "source": [
    "# read the data into a pandas data frame\n",
    "file_descriptors = pd.read_csv(\"/prometheus/GRN/recording_files_with_tags_and_track.csv\")\n",
    "print(f'File descriptors shape: {file_descriptors.shape}')\n",
    "items = pd.read_csv(\"/prometheus/GRN/grid_program_items.csv\")\n",
    "print(f'Program items shape: {items.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File descriptors columns include:\n",
      "['LanguageID', 'ISO', 'Language', 'Program', 'Track', 'Recordist', 'Location', 'Year', 'Path', 'Filename', 'Size', 'Length']\n",
      "------------------------------\n",
      "Program Items columns include:\n",
      "['Program Number', 'Program Item Number', 'Tape Side', 'Track Number', 'Original Recording Number', 'Original Item Number', 'Title', 'Vernacular Item Title', 'Language Number', 'Language Name', 'Item Start Time', 'Item Time', 'Finish Time', 'Original Time', 'Script Number', 'Script Name', 'Picture Number', 'Item Type', 'Comments', 'Entered By', 'Enter On Date']\n"
     ]
    }
   ],
   "source": [
    "print(f'File descriptors columns include:\\n{list(file_descriptors.columns)}')\n",
    "print(\"-\"*30)\n",
    "print(f'Program Items columns include:\\n{list(items.columns)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how many iso codes in the file descriptors? And how many language numbers in the items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iso codes in file descriptors: 3972\n",
      "Number of language numbers in file descriptors: 6148\n",
      "Number of language names in file descriptors: 6130\n",
      "Number of file descriptors without an ISO code: 946\n",
      "Number of language numbers in program items: 6461\n",
      "Number of language names in program items: 6444\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of iso codes in file descriptors: {len(file_descriptors[\"ISO\"].value_counts())}')\n",
    "print(f'Number of language numbers in file descriptors: {len(file_descriptors[\"LanguageID\"].value_counts())}')\n",
    "print(f'Number of language names in file descriptors: {len(file_descriptors[\"Language\"].value_counts())}')\n",
    "print(f'Number of file descriptors without an ISO code: {sum(file_descriptors[\"ISO\"].isna())}')\n",
    "print(f'Number of language numbers in program items: {len(items[\"Language Number\"].value_counts())}')\n",
    "print(f'Number of language names in program items: {len(items[\"Language Name\"].value_counts())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the items an ID\n",
    "items['ID'] = items['Program Number'] + '_' + items['Track Number'].astype(int).apply('{:0>3d}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the file descriptors have 800 more languages than we used for analysis. Why were the files dropped? We need to look at this.\n",
    "\n",
    "Furthermore we have almost 1000 file descriptors without an ISO code. Can we recover an ISO code from the language number?\n",
    "\n",
    "Also some records pointed to non-existent files -  and were dropped. How many of these were there? Did the file really not exists?\n",
    "\n",
    "## 1. How many records had missing files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files verified as existing for 203879 out of 210704 records.\n"
     ]
    }
   ],
   "source": [
    "def check_for_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['Path'] + item_row['Filename'] )\n",
    "\n",
    "file_descriptors['file exists'] = file_descriptors.apply(check_for_file, axis=1)\n",
    "print(f'Files verified as existing for {sum(file_descriptors[\"file exists\"])} out of {file_descriptors.shape[0]} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files = file_descriptors[file_descriptors['file exists'] == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many languages are in these missing files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iso codes in descriptors with missing files: 1400\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of iso codes in descriptors with missing files: {len(missing_files[\"ISO\"].value_counts())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could recover these files how many languages would we recover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ISO codes that overlap with existing data: 778\n",
      "Number of ISO codes not in existing data: 623\n"
     ]
    }
   ],
   "source": [
    "existent_files = file_descriptors[file_descriptors['file exists']]\n",
    "existing_set = set(existent_files['ISO'])\n",
    "missing_set = set(missing_files['ISO'])\n",
    "overlapping_set = existing_set.intersection(missing_set)\n",
    "print(f'Number of ISO codes that overlap with existing data: {len(overlapping_set)}')\n",
    "print(f'Number of ISO codes not in existing data: {len(missing_set)-len(overlapping_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they are very worthwhile recovering: 778 with more data for existing languages and 623 new languages.\n",
    "\n",
    "Lets try and find the files.\n",
    "\n",
    "Are the missing files located in vox-grn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe using a generator\n",
    "def gen_vox_grn():\n",
    "  resp = requests.get('https://raw.githubusercontent.com/johno-source/vox-grn/main/data/vox-grn.json')\n",
    "  vox_dict = json.loads(resp.text)\n",
    "  for iso in vox_dict.keys():\n",
    "    lang_df = pd.json_normalize(vox_dict[iso])\n",
    "    lang_df['iso'] = iso\n",
    "    yield lang_df\n",
    "\n",
    "vox_df = pd.concat(gen_vox_grn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the program ID\n",
    "vox_df['program'] = vox_df['file'].str.extract('\\./Audio_MP3/[0-9]{2}/([0-9]{5})')\n",
    "vox_df['item'] = vox_df['file'].str.extract('\\./Audio_MP3/[0-9]{2}/[0-9]{5}/.*?(0[0-9]{2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files['prog'] = missing_files['Program'].str.extract('[AC]([0-9]{5})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6604\n"
     ]
    }
   ],
   "source": [
    "missing_files['found_prog'] = missing_files['prog'].isin(vox_df['program'])\n",
    "found_files = missing_files[missing_files['found_prog']]\n",
    "print(len(found_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that all but 221 files can potentially be recovered.\n",
    "\n",
    "Let's try and bring them back. See RecoverFilesFromVoxGRN.ipynb for this effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iso codes in found files: 1394\n",
      "Number that overlap 772\n"
     ]
    }
   ],
   "source": [
    "# the number of languages recovered\n",
    "print(f'Number of iso codes in found files: {len(found_files[\"ISO\"].value_counts())}')\n",
    "found_set = set(found_files['ISO'])\n",
    "overlapping_set = existing_set.intersection(found_set)\n",
    "print(f'Number that overlap {len(overlapping_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that gives over 600 more languages and it gives more data to almost 800 other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Files without an ISO code\n",
    "First identify the files that did not have an iso code. Do they have a language id?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files missing an iso code and a language number 0\n"
     ]
    }
   ],
   "source": [
    "missing_iso = file_descriptors[file_descriptors.ISO.isna()]\n",
    "print(f'Number of files missing an iso code and a language number {sum(missing_iso[\"LanguageID\"].isna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - Can we generate a map of language ID to ISO code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_id_to_iso = {}\n",
    "for file in file_descriptors.itertuples():\n",
    "    if isinstance(file.ISO, str):\n",
    "        if file.LanguageID in lang_id_to_iso:\n",
    "            if file.ISO != lang_id_to_iso[file.LanguageID]:\n",
    "                print(f'{file.LanguageID} is mapped to {file.ISO} and {lang_id_to_iso[file.LanguageID]}')\n",
    "        else:\n",
    "            lang_id_to_iso[file.LanguageID] = file.ISO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can we use this map to restore the ISO code for the files that have it missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with ISO recovered: 0\n",
      "Number of files with ISO not recovered: 946\n",
      "Number of Language IDs with an unknown ISO code 69\n"
     ]
    }
   ],
   "source": [
    "recovered_iso = 0\n",
    "unrecovered_iso = 0\n",
    "unrecovered_lang_ids = set()\n",
    "for file in missing_iso.itertuples():\n",
    "    if file.LanguageID in lang_id_to_iso:\n",
    "        recovered_iso += 1\n",
    "    else:\n",
    "        unrecovered_iso += 1\n",
    "        unrecovered_lang_ids.add(file.LanguageID)\n",
    "\n",
    "print(f'Number of files with ISO recovered: {recovered_iso}')\n",
    "print(f'Number of files with ISO not recovered: {unrecovered_iso}')\n",
    "print(f'Number of Language IDs with an unknown ISO code {len(unrecovered_lang_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the GRN database and the internet it would appear that these 69 languages are not ISO languages.\n",
    "\n",
    "Do these programs exist in vox-grn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing ISO programs in vox-grn 11\n"
     ]
    }
   ],
   "source": [
    "missing_iso = missing_iso.copy()\n",
    "missing_iso['prog'] = missing_iso['Program'].str.extract('[AC]([0-9]{5})')\n",
    "missing_iso['in_vox_grn'] = missing_iso['prog'].isin(vox_df['program'])\n",
    "found_iso = missing_iso[missing_iso['in_vox_grn']].copy()\n",
    "print(f'Number of missing ISO programs in vox-grn {len(found_iso)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these all belong to one language which has an ISO code of nan - which has tripped up python.\n",
    "\n",
    "### Conclusion\n",
    "The files with missing ISO codes really do not have an ISO code. They should just be dropped from the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items dropped based on type.\n",
    "Now the program items were filtered based on type. Lets look at their type to see what was excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message                 212534\n",
      "Song                     24371\n",
      "Scripture                14013\n",
      "Message & Song            6085\n",
      "Instrumental              4233\n",
      "Message/Instrumental      2940\n",
      "Announcement               968\n",
      "Bridge                     960\n",
      "Chorus from                610\n",
      "Testimony                  545\n",
      "Scripture Stories          181\n",
      "Song and Scripture          90\n",
      "Undefined                   77\n",
      "Sound Effect                63\n",
      "Poem                        11\n",
      "Name: Item Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "item_types = items['Item Type'].value_counts()\n",
    "print(item_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we kept 'Message', 'Scripture', 'Scripture Stories', 'Testimony' which is probably reasonable.\n",
    "\n",
    "After we filtered out the items how many languages did we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 261457 usable items out of 267681 total items.\n"
     ]
    }
   ],
   "source": [
    "def usable_types(item_row):\n",
    "    unusable_items = ['Instrumental', 'Sound Effect', 'Announcement', 'Bridge']\n",
    "    return item_row['Item Type'] not in unusable_items\n",
    "\n",
    "items['usable'] = items.apply(usable_types, axis=1)\n",
    "usable_items = items[items[\"usable\"]].copy()\n",
    "\n",
    "usable_items.drop(['usable'], inplace=True, axis=1)\n",
    "print(f'There are {usable_items.shape[0]} usable items out of {items.shape[0]} total items.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of language numbers in usable items: 6461\n",
      "Number of language names in usable items: 6444\n",
      "Number of usable items: 261457\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of language numbers in usable items: {len(usable_items[\"Language Number\"].value_counts())}')\n",
    "print(f'Number of language names in usable items: {len(usable_items[\"Language Name\"].value_counts())}')\n",
    "print(f'Number of usable items: {len(usable_items)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we only lost about 100 languages by doing this filtering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a different tack. How many programs and items in Rob's code are not found in our data? Note that Rob's data did NOT accurately give the item numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages with few segments\n",
    "Some languages had very few segments associated with them. Lets find out why. I have two theories to test:\n",
    "1. The item times meant that data was missed\n",
    "2. The vad failed to recognise voice in the language.\n",
    "\n",
    "Lets investigate cjm, nyq, gaw, jeh, hac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of 4 second segments are:\n",
      "Index(['file_name', 'Index', 'iso', 'program', 'location', 'year', 'path',\n",
      "       'filename', 'item_no', 'title', 'start', 'end', 'seg_start', 'seg_stop',\n",
      "       'seg'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read in the seg_4_df file\n",
    "seg_4_df = pd.read_csv(\"../../data/seg_4_df.csv\")\n",
    "print(f'The columns of 4 second segments are:\\n{seg_4_df.columns}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = seg_4_df['iso'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files did cjm have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cjm files: 43\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of cjm files: {sum(file_descriptors.ISO == \"cjm\")}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how did 43 files finish up having only one segment? Lets listen to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94                      C03180A.mp3\n",
      "95                      C03180B.mp3\n",
      "97831    A66147-01-Introduction.wav\n",
      "97832          A66147-02-H�nh_1.wav\n",
      "97833          A66147-03-H�nh_2.wav\n",
      "97834          A66147-04-H�nh_3.wav\n",
      "97835          A66147-05-H�nh_4.wav\n",
      "97836          A66147-06-H�nh_5.wav\n",
      "97837          A66147-07-H�nh_6.wav\n",
      "97838          A66147-08-H�nh_7.wav\n",
      "97839          A66147-09-H�nh_8.wav\n",
      "97840          A66147-10-H�nh_9.wav\n",
      "97841         A66147-11-H�nh_10.wav\n",
      "97842         A66147-12-H�nh_11.wav\n",
      "97843         A66147-13-H�nh_12.wav\n",
      "97844         A66147-14-H�nh_13.wav\n",
      "97845         A66147-15-H�nh_14.wav\n",
      "97846         A66147-16-H�nh_15.wav\n",
      "97847         A66147-17-H�nh_16.wav\n",
      "97848         A66147-18-H�nh_17.wav\n",
      "97849         A66147-19-H�nh_18.wav\n",
      "97850         A66147-20-H�nh_19.wav\n",
      "97851         A66147-21-H�nh_20.wav\n",
      "97852         A66147-22-H�nh_21.wav\n",
      "97853         A66147-23-H�nh_22.wav\n",
      "97854         A66147-24-H�nh_23.wav\n",
      "97855         A66147-25-H�nh_24.wav\n",
      "97856         A66147-26-H�nh_25.wav\n",
      "97857         A66147-27-H�nh_26.wav\n",
      "97858         A66147-28-H�nh_27.wav\n",
      "97859         A66147-29-H�nh_28.wav\n",
      "97860         A66147-30-H�nh_29.wav\n",
      "97861         A66147-31-H�nh_30.wav\n",
      "97862         A66147-32-H�nh_31.wav\n",
      "97863         A66147-33-H�nh_32.wav\n",
      "97864         A66147-34-H�nh_33.wav\n",
      "97865         A66147-35-H�nh_34.wav\n",
      "97866         A66147-36-H�nh_35.wav\n",
      "97867         A66147-37-H�nh_36.wav\n",
      "97868         A66147-38-H�nh_37.wav\n",
      "97869         A66147-39-H�nh_38.wav\n",
      "97870         A66147-40-H�nh_39.wav\n",
      "97871         A66147-41-H�nh_40.wav\n",
      "Name: Filename, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cjm_files = file_descriptors[file_descriptors.ISO == 'cjm'].copy()\n",
    "print(cjm_files.Filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the files all look good. The language is quite abrupt. What about the item meta data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cjm items: 1\n"
     ]
    }
   ],
   "source": [
    "cjm_items = orig_items[orig_items.program == 'A66147']\n",
    "print(f'Number of cjm items: {len(cjm_items)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one item -  where did the others go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cjm items: 41\n"
     ]
    }
   ],
   "source": [
    "cjm_prog_items = items[items['Program Number'] == 'A66147']\n",
    "print(f'Number of cjm items: {len(cjm_prog_items)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the data in items_with_records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with Records that have cjm language: 1\n"
     ]
    }
   ],
   "source": [
    "items_with_records = pd.read_csv('../../data/items_with_records.csv')\n",
    "print(f'Items with Records that have cjm language: {sum(items_with_records.ISO == \"cjm\")}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. The records are lost before items_with_records is written.\n",
    "\n",
    "Rerunning FormProgramTrackFrame.ipynb it can be seen that the file is lost because os.path.isfile cannot cope with the odd characters in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char at 60 65\n",
      "Char at 60 66\n",
      "Char at 60 110\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n",
      "Char at 60 65533\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_for_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['Path'] + item_row['Filename'] )    \n",
    "\n",
    "def check_for_foreign_file(item_row):\n",
    "    # The issue is that some filenames contain the replacement character because the\n",
    "    # filenames have unknonw foreign characters in them.\n",
    "    path_to_file = '/media/programs/' + item_row['Path'] + item_row['Filename'].replace('ufffd', '*')\n",
    "    print(f'Char at 60 {ord(path_to_file[60])}')\n",
    "    return os.path.isfile(path_to_file)    \n",
    "\n",
    "cjm_files['exists'] = cjm_files.apply(check_for_file, axis=1)\n",
    "cjm_files['foreign exists'] = cjm_files.apply(check_for_foreign_file, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: ISO-8859-1 with confidence 0.73\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "result = chardet.detect(cjm_files.iloc[3].Filename.encode())\n",
    "\n",
    "encoding = result['encoding']\n",
    "confidence = result['confidence']\n",
    "\n",
    "print(f\"Encoding: {encoding} with confidence {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/media/programs/Programs/66/66147/A66147/PM-1812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: Windows-1254 with confidence 0.5343954060317209\n"
     ]
    }
   ],
   "source": [
    "result = chardet.detect(files[0].encode())\n",
    "\n",
    "encoding = result['encoding']\n",
    "confidence = result['confidence']\n",
    "\n",
    "print(f\"Encoding: {encoding} with confidence {confidence}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Prevent this running. It causes an error.\n",
    "    file = '/media/programs/' + cjm_files.iloc[3].Path + cjm_files.iloc[3].Filename\n",
    "    file = file.encode('windows-1250')\n",
    "    print(f'{file} exists is {os.path.exists(file)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A66147-15-Hnh_14.wav exists is False\n"
     ]
    }
   ],
   "source": [
    "print(f'{files[0]} exists is {os.path.exists(files[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 36 36 31 34 37 2d 31 35 2d 48 8d 6e 68 5f 31 34 2e 77 61 76\n"
     ]
    }
   ],
   "source": [
    "bin_str = ' '.join(format(ord(byte), 'x') for byte in files[0])\n",
    "print(bin_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A66147-15-HŤnh_14.wav\n"
     ]
    }
   ],
   "source": [
    "# Define the byte sequence as a bytes object\n",
    "byte_seq = b'\\x41\\x36\\x36\\x31\\x34\\x37\\x2d\\x31\\x35\\x2d\\x48\\x8d\\x6e\\x68\\x5f\\x31\\x34\\x2e\\x77\\x61\\x76'\n",
    "\n",
    "# Decode the byte sequence using the Windows-1250 encoding\n",
    "filename = byte_seq.decode('windows-1250')\n",
    "\n",
    "# Print the decoded filename\n",
    "print(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    bin_str = ' '.join(format(ord(byte), 'x') for byte in file)\n",
    "    print(bin_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing a byte by byte comparison of the files string and the one gained from the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41 A\n",
      "36 36 6\n",
      "36 36 6\n",
      "31 31 1\n",
      "34 34 4\n",
      "37 37 7\n",
      "2d 2d -\n",
      "31 31 1\n",
      "35 35 5\n",
      "2d 2d -\n",
      "48 48 H\n",
      "fffd 8d \n",
      "6e 6e n\n",
      "68 68 h\n",
      "5f 5f _\n",
      "31 31 1\n",
      "34 34 4\n",
      "2e 2e .\n",
      "77 77 w\n",
      "61 61 a\n",
      "76 76 v\n"
     ]
    }
   ],
   "source": [
    "for meta_char, file_char in zip(cjm_files.iloc[16].Filename, files[0]):\n",
    "    print(f'{format(ord(meta_char), \"2x\")} {format(ord(file_char), \"x\")} {file_char}')\n",
    "                                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the character in question: The meta data csv file contains the character sequence EF BF BD which is the UTF-8 code for the unicode replacement character which is inserted when an unknown character is found in a string. This character becomes FFFD when read into a dataframe, which is also the Unicode replacement character. The file contains 8D which is either a windows-1250 or 1251 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/programs/Programs/66/66147/A66147/PM-1812/A66147-15-H*nh_14.wav\n",
      "/media/programs/Programs/66/66147/A66147/PM-1812/A66147-15-Hnh_14.wav exists True\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "f = '/media/programs/' + cjm_files.iloc[3].Path + r'A66147-15-H*nh_14.wav'\n",
    "#f = '/media/programs/' + cjm_files.iloc[3].Path + r'A66147*.wav'\n",
    "print(f)\n",
    "files = glob.glob(f)\n",
    "for f in files:\n",
    "    print(f'{f} exists {os.path.exists(f)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char at 60 41\n",
      "Char at 60 42\n",
      "Char at 60 6e\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n",
      "Char at 60 2a\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def check_for_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['Path'] + item_row['Filename'] )    \n",
    "\n",
    "def check_for_foreign_file(item_row):\n",
    "    # The issue is that some filenames contain the replacement character because the\n",
    "    # filenames have unknonw foreign characters in them.\n",
    "    path_to_file = '/media/programs/' + item_row['Path'] + item_row['Filename'].replace('\\ufffd', '*')\n",
    "    print(f'Char at 60 {ord(path_to_file[60]):x}')\n",
    "    files = glob.glob(path_to_file)\n",
    "    return len(files) == 1    \n",
    "\n",
    "cjm_files['exists'] = cjm_files.apply(check_for_file, axis=1)\n",
    "cjm_files['foreign exists'] = cjm_files.apply(check_for_foreign_file, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - with this new knowledge how many files do we have missing from the original data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files verified as existing for 204436 out of 210704 records.\n"
     ]
    }
   ],
   "source": [
    "def check_for_glob_file(item_row):\n",
    "    path_to_file = '/media/programs/' + item_row['Path'] + item_row['Filename'].replace('\\ufffd', '*')\n",
    "    files = glob.glob(path_to_file)\n",
    "    return len(files) == 1    \n",
    "\n",
    "file_descriptors['file exists'] = file_descriptors.apply(check_for_glob_file, axis=1)\n",
    "print(f'Files verified as existing for {sum(file_descriptors[\"file exists\"])} out of {file_descriptors.shape[0]} records.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this recovered another 600 or so files. Good. Now with the data from vox_grn how many do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_missing_files = file_descriptors[file_descriptors['file exists'] == False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6058 found out of 6268\n"
     ]
    }
   ],
   "source": [
    "new_missing_files['prog'] = new_missing_files['Program'].str.extract('[AC]([0-9]{5})')\n",
    "new_missing_files['found_prog'] = new_missing_files['prog'].isin(vox_df['program'])\n",
    "new_found_files = new_missing_files[new_missing_files['found_prog']]\n",
    "print(f'{len(new_found_files)} found out of {len(new_missing_files)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is with the 210 missing files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_missing = new_missing_files[new_missing_files['found_prog'] == False]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some(80) are copy_masters - which I do not have. \n",
    "\n",
    "Looking at some examples:\n",
    "\n",
    "* A37731 - this exists on the disk but the sub dir is MM, not PM-0000 as in the meta data, and the files are mp3 and broken into items. vox_df does not have program 37731\n",
    "* A19741 - only C19741 exists on the disk and that only contains gif files.\n",
    "* A38090 - exists on the disk but the sub dir is PM-0000, not PM-2106 as in the meta data, and there is a wav file for every item.\n",
    "* A38091 - exists on the disk but the sub dir is PM-0000, not PM-2107 as in the meta data, and there is a wav file for every item.\n",
    "* A38101 - exists on the disk but the sub dir is PM-0000, not PM-2106 as in the neta data, and there is a wav file for every item.\n",
    "\n",
    "In other words, these are genuine errors in the meta-data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completeness of meta data\n",
    "This section builds up a description of all the files on the disk and sees which ones are not known by the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(path):\n",
    "    \"\"\"\n",
    "    Find all files in the directory tree rooted at `path`.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.wav') or file.lower().endswith('.mp3'):\n",
    "                yield root, file\n",
    "\n",
    "files_in_dir = pd.DataFrame.from_records(find_files('/media/programs/Programs'), columns=['path', 'filename'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are about 75000 more files on the disk than in the file meta data. Find out which ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dir['file_match'] = files_in_dir['filename'].isin(file_descriptors['Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63445\n"
     ]
    }
   ],
   "source": [
    "files_not_in_meta = files_in_dir[files_in_dir['file_match'] == False]\n",
    "print(len(files_not_in_meta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so lets pull out the program and the track and see if they are in the item data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files_not_in_meta.copy()\n",
    "f['program'] = f['filename'].str.extract('([AC][0-9]{5})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_track(s):\n",
    "    tracks = re.search(r'[AC][0-9]{5}[AB]{,1}-([0-9]{1,3})', s)\n",
    "    if tracks:\n",
    "        return int(tracks.group(1))\n",
    "    tracks = re.search(r'[AC]-{,1}[0-9]{5}([AB])', s)\n",
    "    if tracks:\n",
    "        return 1 if tracks.group(1) == 'A' else 2\n",
    "    return 0\n",
    "\n",
    "f['track'] = f['filename'].apply(extract_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['ID'] = f['program'] + '_' + f['track'].astype(int).apply('{:0>3d}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19295 files have an item.\n"
     ]
    }
   ],
   "source": [
    "f['has_item'] = f['ID'].isin(items['ID'])\n",
    "print(f'{sum(f[\"has_item\"])} files have an item.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this raises some interesting questions:\n",
    "1. If we discarded the file descriptors and only used the files found on the disk how many items would be usable compared to those when the file descriptors were used:\n",
    "\n",
    "    - with just the original file descriptors\n",
    "    - with the original plus vox-grn files?\n",
    "\n",
    "2. How many duplicated files are on the disk?\n",
    "3. What is the coverage like for files on the disk compared to those in grn-vox?\n",
    "\n",
    "Using items would necessitate solving the problem of multiple items in the one file. This is only really a problem if the languages vary between the items. \n",
    "\n",
    "4. How many items belong to one file that has multiple languages?\n",
    "5. Is this a problem for wav files as well as mp3 files?\n",
    "\n",
    "## Duplicate Files on Disk.\n",
    "\n",
    "Lets start by identifying duplicate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 77381\n"
     ]
    }
   ],
   "source": [
    "files_in_dir['program'] = files_in_dir['filename'].str.extract('([A-Za-z]?[0-9]{5})')\n",
    "files_in_dir['track'] = files_in_dir['filename'].apply(extract_track)\n",
    "files_in_dir['ID'] = files_in_dir['program'] + '_' + files_in_dir['track'].astype(int).apply('{:0>3d}'.format)\n",
    "files_in_dir['duplicated'] = files_in_dir['ID'].duplicated(keep='first')\n",
    "print(f'Number of duplicates: {sum(files_in_dir[\"duplicated\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now discard the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_on_disk = files_in_dir[files_in_dir['duplicated'] == False].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usable items using disk files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of usable items with original metadata and voxgrn: 248682\n",
      "The number of usable items with data from disk: 200812\n"
     ]
    }
   ],
   "source": [
    "items_with_records = pd.read_csv(\"../../data/items_with_records_with_voxgrn_files.csv\")\n",
    "fd = pd.read_csv(\"../../data/records_with_voxgrn_files.csv\")\n",
    "\n",
    "usable_items['file on disk'] = usable_items['ID'].isin(f_on_disk['ID'])\n",
    "print(f'The number of usable items with original metadata and voxgrn: {len(items_with_records)}')\n",
    "print(f'The number of usable items with data from disk: {sum(usable_items[\"file on disk\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the loss of 48000 items? Is it an A/C thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of usable items with original metadata and voxgrn: 248682\n",
      "The number of usable items with data from disk: 205231\n"
     ]
    }
   ],
   "source": [
    "usable_items['_ID'] = usable_items['ID'].str.extract('[A-Za-z]?(.*)')\n",
    "f_on_disk['_ID'] = f_on_disk['ID'].str.extract('[A-Za-z]?(.*)')\n",
    "usable_items['file on disk'] = usable_items['_ID'].isin(f_on_disk['_ID'])\n",
    "print(f'The number of usable items with original metadata and voxgrn: {len(items_with_records)}')\n",
    "print(f'The number of usable items with data from disk: {sum(usable_items[\"file on disk\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining missing files are to do with non-conventional naming meaning that the track cannot be extracted from the file name.\n",
    "\n",
    "Can we augment the existing files with data from the disk?\n",
    "\n",
    "What items do not have data found on any disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of items with records is: 248682\n",
      "The number of items that could be added is: 2973\n",
      "The total number of items is: 251655\n"
     ]
    }
   ],
   "source": [
    "usable_items['data in items with rec'] = usable_items['_ID'].isin(items_with_records['ID'])\n",
    "print(f'The number of items with records is: {sum(usable_items[\"data in items with rec\"])}')\n",
    "usable_items['on disk but not in rec'] = usable_items['file on disk'] & ~usable_items['data in items with rec']\n",
    "usable_items['on disk or in rec'] = usable_items['file on disk'] | usable_items['data in items with rec']\n",
    "print(f'The number of items that could be added is: {sum(usable_items[\"on disk but not in rec\"])}')\n",
    "print(f'The total number of items is: {sum(usable_items[\"on disk or in rec\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of languages is: 6076\n"
     ]
    }
   ],
   "source": [
    "usable_items_on_disk = usable_items[usable_items['on disk or in rec']].copy()\n",
    "print(f'The total number of languages is: {len(set(usable_items_on_disk[\"Language Number\"].to_list()))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the issue is that the files on the disk do not have an iso language associated with them. How many do not have an iso that we can infer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_map(lang):\n",
    "    return lang_id_to_iso[lang] if lang in lang_id_to_iso else '***'\n",
    "\n",
    "usable_items['iso'] = usable_items['Language Number'].apply(language_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items with unfound languages: 4445\n",
      "Number of items with files with unfound languages: 1545\n",
      "Number of items with rec with unfound languages: 1125\n",
      "Number of items with file on disk with unfound languages: 1304\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of items with unfound languages: {sum(usable_items[\"iso\"] == \"***\")}')\n",
    "usable_items_on_disk = usable_items[usable_items['on disk or in rec']].copy()\n",
    "usable_items_with_rec = usable_items[usable_items['data in items with rec']].copy()\n",
    "usable_items_file_on_disk = usable_items[usable_items['file on disk']].copy()\n",
    "\n",
    "print(f'Number of items with files with unfound languages: {sum(usable_items_on_disk[\"iso\"] == \"***\")}')\n",
    "print(f'Number of items with rec with unfound languages: {sum(usable_items_with_rec[\"iso\"] == \"***\")}')\n",
    "print(f'Number of items with file on disk with unfound languages: {sum(usable_items_file_on_disk[\"iso\"] == \"***\")}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last question: If I augment the items with records with those found just on the disk - how many items do I recover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable items on disk but not in records: 3112\n",
      "ISO language known for 2692\n"
     ]
    }
   ],
   "source": [
    "on_disk_recoverable = usable_items[usable_items[\"file on disk\"] & ~usable_items[\"data in items with rec\"]]\n",
    "print(f'Usable items on disk but not in records: {len(on_disk_recoverable)}')\n",
    "print(f'ISO language known for {sum(on_disk_recoverable[\"iso\"] != \"***\")}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - lets recover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Program Number', 'Program Item Number', 'Tape Side', 'Track Number',\n",
      "       'Original Recording Number', 'Original Item Number', 'Title',\n",
      "       'Vernacular Item Title', 'Language Number', 'Language Name',\n",
      "       'Item Start Time', 'Item Time', 'Finish Time', 'Original Time',\n",
      "       'Script Number', 'Script Name', 'Picture Number', 'Item Type',\n",
      "       'Comments', 'Entered By', 'Enter On Date', 'ID', 'file on disk', '_ID',\n",
      "       'data in items with rec', 'on disk but not in rec', 'on disk or in rec',\n",
      "       'iso'],\n",
      "      dtype='object')\n",
      "Index(['path', 'filename', 'file_match', 'program', 'track', 'ID',\n",
      "       'duplicated', '_ID'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'iso', 'language_name', 'track', 'location', 'year',\n",
      "       'path', 'filename', 'length', 'program', 'ID'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'iso', 'language_name', 'track', 'location', 'year',\n",
      "       'path', 'filename', 'length', 'ID', 'item', 'title', 'start',\n",
      "       'duration', 'end', 'type', 'program'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(usable_items.columns)\n",
    "print(f_on_disk.columns)\n",
    "print(fd.columns)\n",
    "print(items_with_records.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['program', 'item', 'track', 'title', 'language_name', 'start',\n",
      "       'duration', 'end', 'type', 'ID', '_ID', 'iso'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "recover_candidates = on_disk_recoverable[on_disk_recoverable[\"iso\"] != \"***\"].copy()\n",
    "recover_candidates.drop(columns=['Tape Side', 'Original Recording Number', 'Original Item Number', \n",
    "                                 'Vernacular Item Title', 'Language Number', 'Script Number', 'Script Name', 'Picture Number', 'Entered By', \n",
    "                                 'Enter On Date', 'Comments', 'file on disk', 'data in items with rec',\n",
    "                                 'on disk but not in rec', 'on disk or in rec', 'Original Time'], inplace=True)\n",
    "recover_candidates.rename(columns={'Program Item Number': 'item', 'Title': 'title', 'Item Start Time': 'start', 'Track Number' : 'track',\n",
    "        'Item Time': 'duration', 'Finish Time': 'end', 'Item Type': 'type', 'Program Number' : 'program', 'Language Name' : 'language_name'}, inplace=True)\n",
    "\n",
    "print(recover_candidates.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot recover location or year. Add in path a filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['program_x', 'item', 'track_x', 'title', 'language_name', 'start',\n",
      "       'duration', 'end', 'type', 'ID_x', '_ID', 'iso', 'path', 'filename',\n",
      "       'file_match', 'program_y', 'track_y', 'ID_y', 'duplicated'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "recover_candidates = recover_candidates.dropna(subset=['_ID'])   \n",
    "candidates = pd.merge(recover_candidates, f_on_disk, on='_ID', how='left')\n",
    "print(candidates.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatched programs: 1002\n",
      "Program x na: 0\n",
      "Program y na: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Mismatched programs: {sum(candidates[\"program_x\"] != candidates[\"program_y\"])}')\n",
    "print(f'Program x na: {sum(candidates[\"program_x\"].isna())}')\n",
    "print(f'Program y na: {sum(candidates[\"program_y\"].isna())}')\n",
    "match_err = candidates[candidates[\"program_x\"] != candidates[\"program_y\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatched tracks: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Mismatched tracks: {sum(candidates[\"track_x\"] != candidates[\"track_y\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.drop(columns=['program_y', 'track_y', 'ID_x', 'ID_y', 'file_match', 'duplicated'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['program_x', 'item', 'track_x', 'title', 'language_name', 'start',\n",
      "       'duration', 'end', 'type', '_ID', 'iso', 'path', 'filename'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'iso', 'language_name', 'track', 'location', 'year',\n",
      "       'path', 'filename', 'length', 'ID', 'item', 'title', 'start',\n",
      "       'duration', 'end', 'type', 'program'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(candidates.columns)\n",
    "print(items_with_records.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates['program'] = candidates['program_x'].str.extract('[A-Za-z]?([0-9]{5})').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.rename(columns={ 'track_x' : 'track', '_ID' : 'ID'}, inplace=True)\n",
    "candidates.drop(columns=['program_x'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the path the same\n",
    "candidates['new_path'] = candidates['path'].str.extract('/media/programs/(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates.drop(columns=['path'], inplace=True)\n",
    "candidates.rename(columns={'new_path' : 'path'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_items_with_records = pd.concat([items_with_records, candidates], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_items_with_records.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of items with records are:\n",
      "Index(['iso', 'language_name', 'track', 'location', 'year', 'path', 'filename',\n",
      "       'length', 'ID', 'item', 'title', 'start', 'duration', 'end', 'type',\n",
      "       'program'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "augmented_items_with_records.to_csv(\"../../data/items_with_records_all.csv\")\n",
    "print(f'The columns of items with records are:\\n{augmented_items_with_records.columns}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
