{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovery of files from vox-grn\n",
    "During investigation of the created database it was discovered that many of the files missing from the programs were present in the vox-grn files.\n",
    "The purpose of this notebook is to recover those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data\n",
    "First read in the descriptors of vox-grn and our created data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['iso', 'program', 'location', 'year', 'path', 'filename', 'item_no',\n",
       "       'title', 'start', 'end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now read in the description of the input and remove the unwanted columns and rename the rest to be python attribute names.\n",
    "items = pd.read_csv('/home/jovyan/work/GRN-Notebooks/Data/all_items_for_processing.csv')\n",
    "items = items.drop(columns=['Unnamed: 0', 'LanguageID', 'Language', 'Track', 'Recordist', 'Size', 'Length', 'Tape Side', 'Item Type', 'composite'])\n",
    "items.rename(inplace=True, columns={ 'ISO' : 'iso', 'Location' : 'location', 'Year' : 'year', 'Path' : 'path', 'Filename' : 'filename', 'Title' : 'title', 'Program Item Number' : 'item_no', 'Program' : 'program' })\n",
    "\n",
    "# the existing ID is based on track number rather than item number. We want to use the item number.\n",
    "items['ID'] = items['program'] + '_' + items['item_no'].astype(int).apply('{:0>3d}'.format)\n",
    "\n",
    "items.set_index('ID', inplace=True)\n",
    "items.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File descriptors shape: (210704, 12)\n",
      "Program items shape: (267681, 21)\n"
     ]
    }
   ],
   "source": [
    "# read the data into a pandas data frame\n",
    "file_descriptors = pd.read_csv(\"/prometheus/GRN/recording_files_with_tags_and_track.csv\")\n",
    "print(f'File descriptors shape: {file_descriptors.shape}')\n",
    "items = pd.read_csv(\"/prometheus/GRN/grid_program_items.csv\")\n",
    "print(f'Program items shape: {items.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the missing files from the file descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files verified as existing for 203879 out of 210704 records.\n"
     ]
    }
   ],
   "source": [
    "def check_for_file(item_row):\n",
    "    return os.path.isfile('/media/programs/' + item_row['Path'] + item_row['Filename'] )\n",
    "\n",
    "file_descriptors['file exists'] = file_descriptors.apply(check_for_file, axis=1)\n",
    "print(f'Files verified as existing for {sum(file_descriptors[\"file exists\"])} out of {file_descriptors.shape[0]} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files = file_descriptors[file_descriptors['file exists'] == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the vox-grn data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe using a generator\n",
    "def gen_vox_grn():\n",
    "  resp = requests.get('https://raw.githubusercontent.com/johno-source/vox-grn/main/data/vox-grn.json')\n",
    "  vox_dict = json.loads(resp.text)\n",
    "  for iso in vox_dict.keys():\n",
    "    lang_df = pd.json_normalize(vox_dict[iso])\n",
    "    lang_df['iso'] = iso\n",
    "    yield lang_df\n",
    "\n",
    "vox_df = pd.concat(gen_vox_grn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6604\n"
     ]
    }
   ],
   "source": [
    "# pull out the program ID\n",
    "vox_df['program'] = vox_df['file'].str.extract('\\./Audio_MP3/[0-9]{2}/([0-9]{5})')\n",
    "missing_files['prog'] = missing_files['Program'].str.extract('[AC]([0-9]{5})')\n",
    "missing_files['found_prog'] = missing_files['prog'].isin(vox_df['program'])\n",
    "found_files = missing_files[missing_files['found_prog']]\n",
    "print(len(found_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Recovery from vox-grn\n",
    "\n",
    "First of all we currently have the missing files, but what we really need is the subset of vox_df that we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovery files: 7015\n"
     ]
    }
   ],
   "source": [
    "vox_df['recovery_candidate'] = vox_df['program'].isin(missing_files['prog'])\n",
    "vox_files = vox_df[vox_df['recovery_candidate']]\n",
    "print(f'Recovery files: {len(vox_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up access to the aws server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = getpass.getpass('key')\n",
    "secret = getpass.getpass('secret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "client = session.client('s3',\n",
    "                        region_name='us-east-1',\n",
    "                        aws_access_key_id=access_key,\n",
    "                        aws_secret_access_key=secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vox_grn_key(vox_grn_dir):\n",
    "    return vox_grn_dir[2:]\n",
    "\n",
    "def prepare_vox_grn_file(vox_grn_key):\n",
    "    vox_path_list = vox_grn_key.split('/')\n",
    "    vox_path = '.'\n",
    "    if len(vox_path_list) > 1:\n",
    "        vox_path = '/media/programs/vox_grn/' + '/'.join(vox_path_list[:-1])\n",
    "        Path(vox_path).mkdir(parents=True, exist_ok=True)\n",
    "    return vox_path + '/' + vox_path_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate through all the files that could be recovered and download them from the amazon server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........100 out of 7015 completed in 444.0606791973114 seconds.\n",
      "..........200 out of 7015 completed in 2105.9120016098022 seconds.\n",
      "..........300 out of 7015 completed in 3766.7452392578125 seconds.\n",
      "..........400 out of 7015 completed in 4903.281210899353 seconds.\n",
      "..........500 out of 7015 completed in 6172.015818119049 seconds.\n",
      "..........600 out of 7015 completed in 7420.355749607086 seconds.\n",
      "..........700 out of 7015 completed in 8376.8984105587 seconds.\n",
      "..........800 out of 7015 completed in 9615.405744552612 seconds.\n",
      "..........900 out of 7015 completed in 10789.719019889832 seconds.\n",
      "..........1000 out of 7015 completed in 11266.099214792252 seconds.\n",
      "..........1100 out of 7015 completed in 12378.761924266815 seconds.\n",
      "..........1200 out of 7015 completed in 13178.961907863617 seconds.\n",
      "..........1300 out of 7015 completed in 13633.576668262482 seconds.\n",
      "..........1400 out of 7015 completed in 14511.333701372147 seconds.\n",
      "..........1500 out of 7015 completed in 15624.468387842178 seconds.\n",
      "..........1600 out of 7015 completed in 15978.52331829071 seconds.\n",
      "..........1700 out of 7015 completed in 16862.671813964844 seconds.\n",
      "..........1800 out of 7015 completed in 17541.34271287918 seconds.\n",
      "..........1900 out of 7015 completed in 18684.53031682968 seconds.\n",
      "..........2000 out of 7015 completed in 19447.221213579178 seconds.\n",
      "..........2100 out of 7015 completed in 20225.858582258224 seconds.\n",
      "..........2200 out of 7015 completed in 21232.57914495468 seconds.\n",
      "..........2300 out of 7015 completed in 22245.59818649292 seconds.\n",
      "..........2400 out of 7015 completed in 23289.672142744064 seconds.\n",
      "..........2500 out of 7015 completed in 23906.933103084564 seconds.\n",
      "..........2600 out of 7015 completed in 25112.379272460938 seconds.\n",
      "..........2700 out of 7015 completed in 25960.67049884796 seconds.\n",
      "..........2800 out of 7015 completed in 26276.6597468853 seconds.\n",
      "..........2900 out of 7015 completed in 26921.38568854332 seconds.\n",
      "..........3000 out of 7015 completed in 28000.45522212982 seconds.\n",
      "..........3100 out of 7015 completed in 28394.389646291733 seconds.\n",
      "..........3200 out of 7015 completed in 29083.954322099686 seconds.\n",
      "..........3300 out of 7015 completed in 30362.625690937042 seconds.\n",
      "..........3400 out of 7015 completed in 30879.95037317276 seconds.\n",
      "..........3500 out of 7015 completed in 31831.871819019318 seconds.\n",
      "..........3600 out of 7015 completed in 32315.17550253868 seconds.\n",
      "..........3700 out of 7015 completed in 33474.74982261658 seconds.\n",
      "..........3800 out of 7015 completed in 34559.281111717224 seconds.\n",
      "..........3900 out of 7015 completed in 35659.795766830444 seconds.\n",
      "..........4000 out of 7015 completed in 37045.02832865715 seconds.\n",
      "..........4100 out of 7015 completed in 38320.27414345741 seconds.\n",
      "..........4200 out of 7015 completed in 39271.19335222244 seconds.\n",
      "..........4300 out of 7015 completed in 39571.64605855942 seconds.\n",
      "..........4400 out of 7015 completed in 40161.29874587059 seconds.\n",
      "..........4500 out of 7015 completed in 40673.256620168686 seconds.\n",
      "..........4600 out of 7015 completed in 42017.15053105354 seconds.\n",
      "..........4700 out of 7015 completed in 42775.63253712654 seconds.\n",
      "..........4800 out of 7015 completed in 42964.77140069008 seconds.\n",
      "..........4900 out of 7015 completed in 43220.87533760071 seconds.\n",
      "..........5000 out of 7015 completed in 44143.333629608154 seconds.\n",
      "..........5100 out of 7015 completed in 45529.26185917854 seconds.\n",
      "..........5200 out of 7015 completed in 46620.880034685135 seconds.\n",
      "..........5300 out of 7015 completed in 47445.17954659462 seconds.\n",
      "..........5400 out of 7015 completed in 47635.64755940437 seconds.\n",
      "..........5500 out of 7015 completed in 48071.88268971443 seconds.\n",
      "..........5600 out of 7015 completed in 48752.608890771866 seconds.\n",
      "..........5700 out of 7015 completed in 49994.313551187515 seconds.\n",
      "..........5800 out of 7015 completed in 50955.674369096756 seconds.\n",
      "..........5900 out of 7015 completed in 51951.181874752045 seconds.\n",
      "..........6000 out of 7015 completed in 52706.062799453735 seconds.\n",
      "..........6100 out of 7015 completed in 52958.99085569382 seconds.\n",
      "..........6200 out of 7015 completed in 53522.61042428017 seconds.\n",
      "..........6300 out of 7015 completed in 54341.502811431885 seconds.\n",
      "..........6400 out of 7015 completed in 55321.62359166145 seconds.\n",
      "..........6500 out of 7015 completed in 56526.32300543785 seconds.\n",
      "..........6600 out of 7015 completed in 57463.36970639229 seconds.\n",
      "..........6700 out of 7015 completed in 58286.954792261124 seconds.\n",
      "..........6800 out of 7015 completed in 59005.504251241684 seconds.\n",
      "..........6900 out of 7015 completed in 60223.03838276863 seconds.\n",
      "..........7000 out of 7015 completed in 60939.940150499344 seconds.\n",
      "."
     ]
    }
   ],
   "source": [
    "# now iterate over all the candidates and download them if possible\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "for candidate in vox_files.itertuples():\n",
    "    vox_grn_key = extract_vox_grn_key(candidate.file)\n",
    "    filename = prepare_vox_grn_file(vox_grn_key)\n",
    "    if not os.path.exists(filename):\n",
    "        client.download_file('grn-media',vox_grn_key, filename)\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print('.', end='')\n",
    "    if count % 100 == 0:\n",
    "        print(f'{count} out of {len(vox_files)} completed in {time.time() - start_time} seconds.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - Now that the files have been recovered we need to put them back into the file descriptors. \n",
    "\n",
    "This will be done in FileDescriptorsFromVoxGRN.ipynb.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
