{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a local dataset\n",
    "The purpose of this is to get going with locally generated data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our file description\n",
    "items = pd.read_csv(\"/home/jovyan/work/GRN-Notebooks/Data/items_with_records.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to export half a dozen records as a json file.\n",
    "items_eng = items[items.ISO == 'eng']\n",
    "items_spa = items[items.ISO == 'spa']\n",
    "items_fra = items[items.ISO == 'fra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sample_ds = pd.concat([items_eng.iloc[:2], items_spa.iloc[:2], items_fra.iloc[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'LanguageID', 'ISO', 'Language', 'Program', 'Track',\n",
      "       'Recordist', 'Location', 'Year', 'Path', 'Filename', 'Size', 'Length',\n",
      "       'ID', 'Program Item Number', 'Tape Side', 'Title', 'Item Start Time',\n",
      "       'Item Time', 'Finish Time', 'Original Time', 'Item Type', 'Compound'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(items_sample_ds.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sample_ds.drop(columns=['Unnamed: 0', 'LanguageID', 'Recordist', 'Location', 'Year', 'Program', 'Track', 'Item Start Time', 'Item Time', 'Finish Time', 'Original Time', 'Compound'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(row):\n",
    "    return '/media/programs/' + row['Path'] + row['Filename']\n",
    "\n",
    "items_sample_ds['file'] = items_sample_ds.apply(create_filename, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sample_ds.drop(columns=['Path', 'Filename'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_sample_ds.to_json('./example_ds.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-78652adcc619fa7f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/jovyan/.cache/huggingface/datasets/json/default-78652adcc619fa7f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2687706fc8cb4080b84c4adcef442b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc81f9938b0946e58ec080a9562e59b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113fed70f931433796d845cb0d47ec0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/json/default-78652adcc619fa7f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "sample_ds = load_dataset('json', data_files='./example_ds.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ISO': Value(dtype='string', id=None), 'Language': Value(dtype='string', id=None), 'Size': Value(dtype='int64', id=None), 'Length': Value(dtype='float64', id=None), 'ID': Value(dtype='string', id=None), 'Program Item Number': Value(dtype='int64', id=None), 'Tape Side': Value(dtype='string', id=None), 'Title': Value(dtype='string', id=None), 'Item Type': Value(dtype='string', id=None), 'file': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(sample_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prove the data set on a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce661c86dc0433ba964cedca1ca714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "def map_to_array(example):\n",
    "    speech, _ = librosa.load(example[\"file\"], sr=16000, mono=True)\n",
    "    example[\"speech\"] = speech[:64000]\n",
    "    return example\n",
    "\n",
    "sample_ds = sample_ds.map(map_to_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-large-superb-er\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/hubert-large-superb-er\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute attention masks and normalize the waveform if needed\n",
    "inputs = feature_extractor(sample_ds[\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logits = model(**inputs).logits\n",
    "logits = model(**inputs).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ISO', 'Language', 'Size', 'Length', 'ID', 'Program Item Number', 'Tape Side', 'Title', 'Item Type', 'file'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print( sample_ds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the different json export formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def show_json(orient):\n",
    "    global items_sample_ds\n",
    "    result = items_sample_ds.to_json( orient=orient)\n",
    "    parsed = json.loads(result)\n",
    "    print(json.dumps(parsed, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"ISO\": \"eng\",\n",
      "        \"Language\": \"English: Aboriginal\",\n",
      "        \"Size\": 5365806,\n",
      "        \"Length\": 37.259903,\n",
      "        \"ID\": \"A75295_001\",\n",
      "        \"Program Item Number\": 1,\n",
      "        \"Tape Side\": \" \",\n",
      "        \"Title\": \"Picture 1\",\n",
      "        \"Item Type\": \"Message\",\n",
      "        \"file\": \"/media/programs/Programs/75/75295/A75295/PM-1811/A75295-001.wav\"\n",
      "    },\n",
      "    {\n",
      "        \"ISO\": \"eng\",\n",
      "        \"Language\": \"English: Aboriginal\",\n",
      "        \"Size\": 3665474,\n",
      "        \"Length\": 25.451528,\n",
      "        \"ID\": \"A75295_002\",\n",
      "        \"Program Item Number\": 2,\n",
      "        \"Tape Side\": \" \",\n",
      "        \"Title\": \"Picture 2\",\n",
      "        \"Item Type\": \"Message\",\n",
      "        \"file\": \"/media/programs/Programs/75/75295/A75295/PM-1811/A75295-002.wav\"\n",
      "    },\n",
      "    {\n",
      "        \"ISO\": \"spa\",\n",
      "        \"Language\": \"Spanish: Mexico\",\n",
      "        \"Size\": 219330154,\n",
      "        \"Length\": 1523.0,\n",
      "        \"ID\": \"A66344_001\",\n",
      "        \"Program Item Number\": 1,\n",
      "        \"Tape Side\": \" \",\n",
      "        \"Title\": \"The Refugee's Journey\",\n",
      "        \"Item Type\": \"Message\",\n",
      "        \"file\": \"/media/programs/Programs/66/66344/A66344/PM-1909/A66344-001.wav\"\n",
      "    },\n",
      "    {\n",
      "        \"ISO\": \"spa\",\n",
      "        \"Language\": \"Spanish: Cuba\",\n",
      "        \"Size\": 11264136,\n",
      "        \"Length\": 117.333354,\n",
      "        \"ID\": \"A37652_001\",\n",
      "        \"Program Item Number\": 2,\n",
      "        \"Tape Side\": \" \",\n",
      "        \"Title\": \"Praise the Lord\",\n",
      "        \"Item Type\": \"Message\",\n",
      "        \"file\": \"/media/programs/Programs/37/37652/A37652/PM-0808/A37652-01.wav\"\n",
      "    },\n",
      "    {\n",
      "        \"ISO\": \"fra\",\n",
      "        \"Language\": \"French: Africa\",\n",
      "        \"Size\": 70742156,\n",
      "        \"Length\": 736.896,\n",
      "        \"ID\": \"A27311_002\",\n",
      "        \"Program Item Number\": 2,\n",
      "        \"Tape Side\": \" \",\n",
      "        \"Title\": \"Testimony\",\n",
      "        \"Item Type\": \"Testimony\",\n",
      "        \"file\": \"/media/programs/Programs/27/27311/A27311/From_CM_au/A27311-02.wav\"\n",
      "    },\n",
      "    {\n",
      "        \"ISO\": \"fra\",\n",
      "        \"Language\": \"French\",\n",
      "        \"Size\": 324380924,\n",
      "        \"Length\": 1838.893333,\n",
      "        \"ID\": \"C37954_001\",\n",
      "        \"Program Item Number\": 4,\n",
      "        \"Tape Side\": \"A\",\n",
      "        \"Title\": \"Text\",\n",
      "        \"Item Type\": \"Message\",\n",
      "        \"file\": \"/media/programs/Programs/37/37954/C37954/PM/C37954-01.wav\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "show_json('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the json file written from a pandas data frame needs to be editted before it can be used by datasets. How can this be addressed?\n",
    "\n",
    "Records is the closest format.\n",
    "\n",
    "Try a different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ISO': 'eng', 'Language': 'English: Aboriginal', 'Size': 5365806, 'Length': 37.259903, 'ID': 'A75295_001', 'Program Item Number': 1, 'Tape Side': ' ', 'Title': 'Picture 1', 'Item Type': 'Message', 'file': '/media/programs/Programs/75/75295/A75295/PM-1811/A75295-001.wav', '__index_level_0__': 205}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(items_sample_ds)\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912ea5ad98d4d359d29ef6d5e4fafda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1708"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_json('../../data/vox6.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2085ca88d9373de1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/jovyan/.cache/huggingface/datasets/json/default-2085ca88d9373de1/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108256c5d93040e08401b3a9ec1d4e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fe0e3a046143d78a8941f324c2377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e26fa0390b84a849660b1a58ef89bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/json/default-2085ca88d9373de1/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "try_ds = load_dataset('json', data_files='../../data/vox6.json', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works! So the process is to convert a pandas from to a dataset and then write the dataset to json. The json can be read back without problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
